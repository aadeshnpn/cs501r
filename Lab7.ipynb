{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OvmxaTDQCQFZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Installation\n"
      ]
    },
    {
      "metadata": {
        "id": "BX7h4aeuCVYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1406
        },
        "outputId": "7366b6c8-46e3-4683-ee0e-e04da6ed7673"
      },
      "cell_type": "code",
      "source": [
        "!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext==0.2.3 seaborn\n",
        "  \n",
        "!python -m spacy download en\n",
        "!python -m spacy download de\n",
        "!python -m spacy download es"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4 from http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 592.3MB 66.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.16)\n",
            "Collecting torchtext==0.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.27.0)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy<0.4.4->spacy) (0.5.6)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.10.15)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy) (0.9.0)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torch, torchtext\n",
            "Successfully installed torch-0.3.0.post4 torchtext-0.2.3\n",
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 46.9MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 38.2MB 43.8MB/s \n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "  Running setup.py install for de-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "\n",
            "    You can now load the model via spacy.load('de')\n",
            "\n",
            "Collecting es_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz#egg=es_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz (36.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 36.7MB 49.0MB/s \n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "  Running setup.py install for es-core-news-sm ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25hSuccessfully installed es-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "\n",
            "    You can now load the model via spacy.load('es')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uner5bIpCa0H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import"
      ]
    },
    {
      "metadata": {
        "id": "79PbiDu6CaF_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Qp9yB_FCaab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "metadata": {
        "id": "DgZFJdflCY_h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "      \n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "      \n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "  \n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "      \n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "      \n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "      \n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "      \n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "      \n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "      \n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nGveviXxNORk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attentation"
      ]
    },
    {
      "metadata": {
        "id": "L2RIoIcLCjvO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "  \n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)  \n",
        "      \n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "      \n",
        "      \n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)      \n",
        "      \n",
        "      \n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zQcC30FlNg-w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "RUsZ9fZhNhtE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model\n",
        "  \n",
        "  \n",
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5E5yfdQeNoHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "_BsvjlEqNpRN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens\n",
        "  \n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n",
        "  \n",
        "\n",
        "\n",
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "  \n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        \n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "      \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vw2anwlIOKKa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data"
      ]
    },
    {
      "metadata": {
        "id": "OExnvla7OLSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_gen(V, batch, nbatches):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        data[:, 0] = 1\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        yield Batch(src, tgt, 0)\n",
        "        \n",
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        \n",
        "        if self.opt is not None:\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return loss.data[0] * norm        \n",
        "      \n",
        "      \n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys\n",
        "\n",
        "# model.eval()\n",
        "# src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
        "# src_mask = Variable(torch.ones(1, 1, 10) )\n",
        "# print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))      \n",
        "\n",
        "# For data loading.\n",
        "from torchtext import data, datasets\n",
        "\n",
        "if True:\n",
        "    import spacy\n",
        "    # spacy_de = spacy.load('de')\n",
        "    # spacy_en = spacy.load('en')\n",
        "    # spacy_es = spacy.load('es')\n",
        "\n",
        "    \n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "      \n",
        "    def tokenize_es(text):\n",
        "        return [tok.text for tok in spacy_es.tokenizer(text)]      \n",
        "    \"\"\"\n",
        "    BOS_WORD = '<s>'\n",
        "    EOS_WORD = '</s>'\n",
        "    BLANK_WORD = \"<blank>\"\n",
        "    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                     eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "    MAX_LEN = 100\n",
        "    train, val, test = datasets.IWSLT.splits(\n",
        "        exts=('.de', '.en'), fields=(SRC, TGT), \n",
        "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "            len(vars(x)['trg']) <= MAX_LEN)\n",
        "    MIN_FREQ = 2\n",
        "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
        "    \"\"\"\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_FpgVw9fAEB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conference Dataset"
      ]
    },
    {
      "metadata": {
        "id": "282GjrRmfDWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "fd35eb2b-71aa-4d87-ca1b-4fe842f5e7b1"
      },
      "cell_type": "code",
      "source": [
        "## Conference dataset\n",
        "\n",
        "# Download the conference dataset\n",
        "!wget -O es-en-general-conference.tar.gz http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2018:es-en-general-conference.tar.gz\n",
        "!tar -xvf es-en-general-conference.tar.gz    \n",
        "!mv en-es_gc_2010-2017_es.txt en-es_gc_2010-2017.es\n",
        "!mv en-es_gc_2010-2017_en.txt en-es_gc_2010-2017.en\n",
        "!echo \"$PWD\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-21 01:59:31--  http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2018:es-en-general-conference.tar.gz\n",
            "Resolving liftothers.org (liftothers.org)... 50.62.229.1\n",
            "Connecting to liftothers.org (liftothers.org)|50.62.229.1|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18318204 (17M) [application/octet-stream]\n",
            "Saving to: ‘es-en-general-conference.tar.gz’\n",
            "\n",
            "es-en-general-confe 100%[===================>]  17.47M  7.47MB/s    in 2.3s    \n",
            "\n",
            "2018-10-21 01:59:34 (7.47 MB/s) - ‘es-en-general-conference.tar.gz’ saved [18318204/18318204]\n",
            "\n",
            "en-es_gc_2010-2017_en.txt\n",
            "en-es_conference.csv\n",
            "en-es_gc_2010-2017_es.txt\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0rLbOcK_fEW7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\"\"\"\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "SRC = data.Field()\n",
        "TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n",
        "\n",
        "MAX_LEN = 200\n",
        "train = datasets.TranslationDataset(path=\"/content/en-es_gc_2010-2017\", \n",
        "                                    exts=('.es', '.en'),\n",
        "                                    fields=(SRC, TGT), \n",
        "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
        "val = datasets.TranslationDataset(path=\"/content/en-es_gc_2010-2017\", \n",
        "                                    exts=('.es', '.en'),\n",
        "                                    fields=(SRC, TGT), \n",
        "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
        "\n",
        "SRC.build_vocab(train.src, max_size=5000)\n",
        "TGT.build_vocab(train.trg, max_size=5000)\n",
        "\n",
        "\n",
        "#\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4QxAcRSrfYiv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Make model"
      ]
    },
    {
      "metadata": {
        "id": "-NWAPClDfaNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8af4015-e04e-40ff-82da-b93241fec89a"
      },
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "    print(len(SRC.vocab))\n",
        "    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "    model.cuda()\n",
        "    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "    criterion.cuda()\n",
        "    BATCH_SIZE = 200\n",
        "    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=False)\n",
        "    # model_par = nn.DataParallel(model, device_ids=devices)\n",
        "None"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5XJOsa1tPKjq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "2WDTyEoBPLqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18873
        },
        "outputId": "6452ce10-67d4-41d4-db0e-e627f43ea53f"
      },
      "cell_type": "code",
      "source": [
        "# GPUs to use\n",
        "# devices = [0, 1, 2, 3]\n",
        "\n",
        "if True:\n",
        "    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 200,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "    for epoch in range(5):\n",
        "        model.train()\n",
        "        run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "                  model, \n",
        "                  SimpleLossCompute(model.generator, criterion, \n",
        "                                       opt=model_opt))\n",
        "        model.eval()\n",
        "        \n",
        "        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "                          model, \n",
        "                          SimpleLossCompute(model.generator, criterion, \n",
        "                           opt=None))\n",
        "        \n",
        "        torch.save(model,'transformer'+str(epoch)+'.pt')\n",
        "        \n",
        "        print(loss)\n",
        "else:\n",
        "    model = torch.load(\"transformer1.pt\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 1 Loss: 7.346594 Tokens per Sec: 370.997708\n",
            "Epoch Step: 51 Loss: 5.190081 Tokens per Sec: 857.552790\n",
            "Epoch Step: 101 Loss: 4.984794 Tokens per Sec: 857.246092\n",
            "Epoch Step: 151 Loss: 5.451591 Tokens per Sec: 853.792822\n",
            "Epoch Step: 201 Loss: 5.140787 Tokens per Sec: 808.745665\n",
            "Epoch Step: 251 Loss: 5.206261 Tokens per Sec: 824.100145\n",
            "Epoch Step: 301 Loss: 5.158212 Tokens per Sec: 804.532311\n",
            "Epoch Step: 351 Loss: 5.130233 Tokens per Sec: 866.694722\n",
            "Epoch Step: 401 Loss: 4.910839 Tokens per Sec: 843.598655\n",
            "Epoch Step: 451 Loss: 4.702432 Tokens per Sec: 832.287978\n",
            "Epoch Step: 501 Loss: 4.915559 Tokens per Sec: 871.221694\n",
            "Epoch Step: 551 Loss: 4.481764 Tokens per Sec: 801.772670\n",
            "Epoch Step: 601 Loss: 5.112978 Tokens per Sec: 833.017852\n",
            "Epoch Step: 651 Loss: 5.199763 Tokens per Sec: 860.820503\n",
            "Epoch Step: 701 Loss: 4.651597 Tokens per Sec: 848.072906\n",
            "Epoch Step: 751 Loss: 4.948694 Tokens per Sec: 831.908013\n",
            "Epoch Step: 801 Loss: 4.679266 Tokens per Sec: 839.466184\n",
            "Epoch Step: 851 Loss: 4.212921 Tokens per Sec: 838.053837\n",
            "Epoch Step: 901 Loss: 4.880805 Tokens per Sec: 837.844532\n",
            "Epoch Step: 951 Loss: 4.603402 Tokens per Sec: 838.355720\n",
            "Epoch Step: 1001 Loss: 4.735317 Tokens per Sec: 807.094764\n",
            "Epoch Step: 1051 Loss: 4.719781 Tokens per Sec: 831.813568\n",
            "Epoch Step: 1101 Loss: 4.663492 Tokens per Sec: 814.275754\n",
            "Epoch Step: 1151 Loss: 4.801237 Tokens per Sec: 819.430231\n",
            "Epoch Step: 1201 Loss: 4.642022 Tokens per Sec: 844.944230\n",
            "Epoch Step: 1251 Loss: 4.472976 Tokens per Sec: 832.460037\n",
            "Epoch Step: 1301 Loss: 4.773925 Tokens per Sec: 819.137438\n",
            "Epoch Step: 1351 Loss: 4.685534 Tokens per Sec: 869.583571\n",
            "Epoch Step: 1401 Loss: 4.638833 Tokens per Sec: 813.195152\n",
            "Epoch Step: 1451 Loss: 4.909334 Tokens per Sec: 837.027475\n",
            "Epoch Step: 1501 Loss: 4.814732 Tokens per Sec: 812.022557\n",
            "Epoch Step: 1551 Loss: 4.296227 Tokens per Sec: 840.701630\n",
            "Epoch Step: 1601 Loss: 4.365383 Tokens per Sec: 860.839297\n",
            "Epoch Step: 1651 Loss: 4.286337 Tokens per Sec: 786.811279\n",
            "Epoch Step: 1701 Loss: 4.574182 Tokens per Sec: 818.824252\n",
            "Epoch Step: 1751 Loss: 4.582104 Tokens per Sec: 823.488025\n",
            "Epoch Step: 1801 Loss: 5.128342 Tokens per Sec: 821.028087\n",
            "Epoch Step: 1851 Loss: 4.809687 Tokens per Sec: 852.462870\n",
            "Epoch Step: 1901 Loss: 4.451735 Tokens per Sec: 825.887296\n",
            "Epoch Step: 1951 Loss: 4.863245 Tokens per Sec: 826.746733\n",
            "Epoch Step: 2001 Loss: 3.974909 Tokens per Sec: 834.278213\n",
            "Epoch Step: 2051 Loss: 4.994154 Tokens per Sec: 873.058750\n",
            "Epoch Step: 2101 Loss: 4.978407 Tokens per Sec: 849.357139\n",
            "Epoch Step: 2151 Loss: 4.529571 Tokens per Sec: 849.273076\n",
            "Epoch Step: 2201 Loss: 4.915095 Tokens per Sec: 824.122708\n",
            "Epoch Step: 2251 Loss: 5.106449 Tokens per Sec: 825.844645\n",
            "Epoch Step: 2301 Loss: 4.660288 Tokens per Sec: 827.332716\n",
            "Epoch Step: 2351 Loss: 4.929059 Tokens per Sec: 834.353340\n",
            "Epoch Step: 2401 Loss: 2.355000 Tokens per Sec: 852.833292\n",
            "Epoch Step: 2451 Loss: 4.352613 Tokens per Sec: 861.130123\n",
            "Epoch Step: 2501 Loss: 4.292134 Tokens per Sec: 816.638513\n",
            "Epoch Step: 2551 Loss: 4.465630 Tokens per Sec: 854.923691\n",
            "Epoch Step: 2601 Loss: 4.802746 Tokens per Sec: 838.787843\n",
            "Epoch Step: 2651 Loss: 4.393101 Tokens per Sec: 858.682521\n",
            "Epoch Step: 2701 Loss: 4.427625 Tokens per Sec: 816.913367\n",
            "Epoch Step: 2751 Loss: 4.484135 Tokens per Sec: 854.923631\n",
            "Epoch Step: 2801 Loss: 4.887639 Tokens per Sec: 845.109963\n",
            "Epoch Step: 2851 Loss: 4.439684 Tokens per Sec: 803.070487\n",
            "Epoch Step: 2901 Loss: 3.832403 Tokens per Sec: 820.079382\n",
            "Epoch Step: 2951 Loss: 4.623262 Tokens per Sec: 821.095184\n",
            "Epoch Step: 3001 Loss: 5.097702 Tokens per Sec: 821.790241\n",
            "Epoch Step: 3051 Loss: 4.837196 Tokens per Sec: 803.283914\n",
            "Epoch Step: 3101 Loss: 5.082302 Tokens per Sec: 845.620986\n",
            "Epoch Step: 3151 Loss: 4.917423 Tokens per Sec: 828.736201\n",
            "Epoch Step: 3201 Loss: 4.435191 Tokens per Sec: 810.934448\n",
            "Epoch Step: 3251 Loss: 4.197631 Tokens per Sec: 808.901608\n",
            "Epoch Step: 3301 Loss: 4.233008 Tokens per Sec: 862.714737\n",
            "Epoch Step: 3351 Loss: 4.467999 Tokens per Sec: 816.856952\n",
            "Epoch Step: 3401 Loss: 4.548327 Tokens per Sec: 824.414510\n",
            "Epoch Step: 3451 Loss: 4.862158 Tokens per Sec: 804.307619\n",
            "Epoch Step: 3501 Loss: 4.606869 Tokens per Sec: 818.911002\n",
            "Epoch Step: 3551 Loss: 4.259409 Tokens per Sec: 793.587814\n",
            "Epoch Step: 3601 Loss: 4.651924 Tokens per Sec: 835.211716\n",
            "Epoch Step: 3651 Loss: 4.455767 Tokens per Sec: 810.812792\n",
            "Epoch Step: 3701 Loss: 4.414188 Tokens per Sec: 868.102754\n",
            "Epoch Step: 3751 Loss: 4.793008 Tokens per Sec: 853.018682\n",
            "Epoch Step: 3801 Loss: 4.564020 Tokens per Sec: 808.684635\n",
            "Epoch Step: 3851 Loss: 5.045942 Tokens per Sec: 843.073667\n",
            "Epoch Step: 3901 Loss: 4.773690 Tokens per Sec: 815.263387\n",
            "Epoch Step: 3951 Loss: 4.740815 Tokens per Sec: 833.436092\n",
            "Epoch Step: 4001 Loss: 4.504596 Tokens per Sec: 824.004399\n",
            "Epoch Step: 4051 Loss: 4.498874 Tokens per Sec: 837.451814\n",
            "Epoch Step: 4101 Loss: 4.347252 Tokens per Sec: 811.579641\n",
            "Epoch Step: 4151 Loss: 4.583119 Tokens per Sec: 816.147699\n",
            "Epoch Step: 4201 Loss: 4.536373 Tokens per Sec: 831.513578\n",
            "Epoch Step: 4251 Loss: 4.871202 Tokens per Sec: 832.496817\n",
            "Epoch Step: 4301 Loss: 4.359836 Tokens per Sec: 807.109387\n",
            "Epoch Step: 4351 Loss: 4.673961 Tokens per Sec: 834.804348\n",
            "Epoch Step: 4401 Loss: 1.882195 Tokens per Sec: 836.059393\n",
            "Epoch Step: 4451 Loss: 4.864469 Tokens per Sec: 825.492780\n",
            "Epoch Step: 4501 Loss: 4.606392 Tokens per Sec: 846.837954\n",
            "Epoch Step: 4551 Loss: 4.474381 Tokens per Sec: 826.354485\n",
            "Epoch Step: 4601 Loss: 4.626415 Tokens per Sec: 841.501430\n",
            "Epoch Step: 4651 Loss: 4.856220 Tokens per Sec: 829.512897\n",
            "Epoch Step: 4701 Loss: 4.816252 Tokens per Sec: 833.695708\n",
            "Epoch Step: 4751 Loss: 4.239495 Tokens per Sec: 863.450485\n",
            "Epoch Step: 4801 Loss: 4.498401 Tokens per Sec: 839.341588\n",
            "Epoch Step: 4851 Loss: 4.660402 Tokens per Sec: 855.853904\n",
            "Epoch Step: 4901 Loss: 4.489363 Tokens per Sec: 831.618826\n",
            "Epoch Step: 4951 Loss: 4.752537 Tokens per Sec: 839.190238\n",
            "Epoch Step: 5001 Loss: 4.728231 Tokens per Sec: 800.315602\n",
            "Epoch Step: 5051 Loss: 2.636090 Tokens per Sec: 826.230296\n",
            "Epoch Step: 5101 Loss: 5.016567 Tokens per Sec: 816.200369\n",
            "Epoch Step: 5151 Loss: 2.871741 Tokens per Sec: 804.546901\n",
            "Epoch Step: 5201 Loss: 4.638836 Tokens per Sec: 835.124156\n",
            "Epoch Step: 5251 Loss: 4.345613 Tokens per Sec: 858.073233\n",
            "Epoch Step: 5301 Loss: 4.643148 Tokens per Sec: 817.580089\n",
            "Epoch Step: 5351 Loss: 4.938574 Tokens per Sec: 861.895795\n",
            "Epoch Step: 5401 Loss: 4.396746 Tokens per Sec: 870.470493\n",
            "Epoch Step: 5451 Loss: 4.420556 Tokens per Sec: 838.710016\n",
            "Epoch Step: 5501 Loss: 4.302386 Tokens per Sec: 827.905795\n",
            "Epoch Step: 5551 Loss: 4.615362 Tokens per Sec: 795.244942\n",
            "Epoch Step: 5601 Loss: 4.081047 Tokens per Sec: 832.075428\n",
            "Epoch Step: 5651 Loss: 3.921726 Tokens per Sec: 831.238236\n",
            "Epoch Step: 5701 Loss: 4.540166 Tokens per Sec: 842.305041\n",
            "Epoch Step: 5751 Loss: 4.383730 Tokens per Sec: 826.352587\n",
            "Epoch Step: 5801 Loss: 4.764688 Tokens per Sec: 795.515172\n",
            "Epoch Step: 5851 Loss: 4.721606 Tokens per Sec: 848.233741\n",
            "Epoch Step: 5901 Loss: 4.822527 Tokens per Sec: 840.789557\n",
            "Epoch Step: 5951 Loss: 4.680620 Tokens per Sec: 776.246988\n",
            "Epoch Step: 6001 Loss: 4.338912 Tokens per Sec: 842.510797\n",
            "Epoch Step: 6051 Loss: 4.348783 Tokens per Sec: 841.494711\n",
            "Epoch Step: 6101 Loss: 4.589202 Tokens per Sec: 857.633737\n",
            "Epoch Step: 6151 Loss: 4.556833 Tokens per Sec: 839.011738\n",
            "Epoch Step: 6201 Loss: 4.472896 Tokens per Sec: 824.496571\n",
            "Epoch Step: 6251 Loss: 4.109295 Tokens per Sec: 832.487382\n",
            "Epoch Step: 6301 Loss: 4.537317 Tokens per Sec: 771.804458\n",
            "Epoch Step: 6351 Loss: 4.554842 Tokens per Sec: 822.615422\n",
            "Epoch Step: 6401 Loss: 4.727909 Tokens per Sec: 856.814166\n",
            "Epoch Step: 6451 Loss: 4.894874 Tokens per Sec: 833.779403\n",
            "Epoch Step: 6501 Loss: 4.479017 Tokens per Sec: 804.112325\n",
            "Epoch Step: 6551 Loss: 4.391175 Tokens per Sec: 841.975264\n",
            "Epoch Step: 6601 Loss: 4.155573 Tokens per Sec: 857.471890\n",
            "Epoch Step: 6651 Loss: 3.843362 Tokens per Sec: 806.580998\n",
            "Epoch Step: 6701 Loss: 4.198160 Tokens per Sec: 839.304081\n",
            "Epoch Step: 6751 Loss: 3.895398 Tokens per Sec: 839.128757\n",
            "Epoch Step: 6801 Loss: 4.836436 Tokens per Sec: 805.982075\n",
            "Epoch Step: 6851 Loss: 4.179282 Tokens per Sec: 838.999752\n",
            "Epoch Step: 6901 Loss: 5.048971 Tokens per Sec: 850.001338\n",
            "Epoch Step: 6951 Loss: 4.141142 Tokens per Sec: 884.262983\n",
            "Epoch Step: 7001 Loss: 4.473174 Tokens per Sec: 830.350015\n",
            "Epoch Step: 7051 Loss: 4.780790 Tokens per Sec: 855.689999\n",
            "Epoch Step: 7101 Loss: 4.403009 Tokens per Sec: 831.648348\n",
            "Epoch Step: 7151 Loss: 4.730264 Tokens per Sec: 812.494316\n",
            "Epoch Step: 7201 Loss: 4.741683 Tokens per Sec: 838.336194\n",
            "Epoch Step: 7251 Loss: 4.605136 Tokens per Sec: 828.676931\n",
            "Epoch Step: 7301 Loss: 4.548574 Tokens per Sec: 833.689336\n",
            "Epoch Step: 7351 Loss: 4.704529 Tokens per Sec: 824.999028\n",
            "Epoch Step: 7401 Loss: 4.541900 Tokens per Sec: 830.558653\n",
            "Epoch Step: 7451 Loss: 4.477862 Tokens per Sec: 814.259054\n",
            "Epoch Step: 7501 Loss: 4.378315 Tokens per Sec: 831.067965\n",
            "Epoch Step: 7551 Loss: 4.393554 Tokens per Sec: 875.036719\n",
            "Epoch Step: 7601 Loss: 4.016030 Tokens per Sec: 826.917759\n",
            "Epoch Step: 7651 Loss: 4.801550 Tokens per Sec: 830.974898\n",
            "Epoch Step: 7701 Loss: 4.530051 Tokens per Sec: 824.027510\n",
            "Epoch Step: 7751 Loss: 4.488818 Tokens per Sec: 833.481181\n",
            "Epoch Step: 7801 Loss: 4.065387 Tokens per Sec: 838.571652\n",
            "Epoch Step: 7851 Loss: 4.409868 Tokens per Sec: 808.035360\n",
            "Epoch Step: 7901 Loss: 4.285156 Tokens per Sec: 795.144272\n",
            "Epoch Step: 7951 Loss: 4.294441 Tokens per Sec: 821.829426\n",
            "Epoch Step: 8001 Loss: 4.449223 Tokens per Sec: 789.019449\n",
            "Epoch Step: 8051 Loss: 4.674997 Tokens per Sec: 829.765695\n",
            "Epoch Step: 8101 Loss: 4.751899 Tokens per Sec: 808.034952\n",
            "Epoch Step: 8151 Loss: 4.411881 Tokens per Sec: 826.497682\n",
            "Epoch Step: 8201 Loss: 4.680974 Tokens per Sec: 828.406108\n",
            "Epoch Step: 8251 Loss: 4.648029 Tokens per Sec: 855.641758\n",
            "Epoch Step: 8301 Loss: 4.750492 Tokens per Sec: 803.306999\n",
            "Epoch Step: 8351 Loss: 4.282447 Tokens per Sec: 835.341486\n",
            "Epoch Step: 8401 Loss: 4.830238 Tokens per Sec: 819.575758\n",
            "Epoch Step: 8451 Loss: 4.418943 Tokens per Sec: 819.425282\n",
            "Epoch Step: 8501 Loss: 4.793216 Tokens per Sec: 843.775418\n",
            "Epoch Step: 8551 Loss: 4.626012 Tokens per Sec: 835.953150\n",
            "Epoch Step: 8601 Loss: 1.699627 Tokens per Sec: 847.741304\n",
            "Epoch Step: 8651 Loss: 4.015051 Tokens per Sec: 851.647584\n",
            "Epoch Step: 8701 Loss: 4.347116 Tokens per Sec: 816.258678\n",
            "Epoch Step: 8751 Loss: 4.602704 Tokens per Sec: 824.733782\n",
            "Epoch Step: 8801 Loss: 4.472747 Tokens per Sec: 810.183094\n",
            "Epoch Step: 8851 Loss: 4.311343 Tokens per Sec: 821.438943\n",
            "Epoch Step: 8901 Loss: 4.977602 Tokens per Sec: 819.680241\n",
            "Epoch Step: 8951 Loss: 4.404476 Tokens per Sec: 784.722895\n",
            "Epoch Step: 9001 Loss: 3.978128 Tokens per Sec: 860.451562\n",
            "Epoch Step: 9051 Loss: 4.162964 Tokens per Sec: 846.375536\n",
            "Epoch Step: 9101 Loss: 4.747719 Tokens per Sec: 836.164731\n",
            "Epoch Step: 9151 Loss: 4.281300 Tokens per Sec: 852.066970\n",
            "Epoch Step: 9201 Loss: 4.511627 Tokens per Sec: 782.592756\n",
            "Epoch Step: 9251 Loss: 3.988160 Tokens per Sec: 839.647393\n",
            "Epoch Step: 9301 Loss: 4.355029 Tokens per Sec: 865.927952\n",
            "Epoch Step: 9351 Loss: 4.684531 Tokens per Sec: 834.513790\n",
            "Epoch Step: 9401 Loss: 4.789191 Tokens per Sec: 825.075683\n",
            "Epoch Step: 9451 Loss: 4.390440 Tokens per Sec: 835.266027\n",
            "Epoch Step: 9501 Loss: 1.883565 Tokens per Sec: 818.954781\n",
            "Epoch Step: 9551 Loss: 4.411975 Tokens per Sec: 839.416568\n",
            "Epoch Step: 9601 Loss: 4.441781 Tokens per Sec: 796.119004\n",
            "Epoch Step: 9651 Loss: 4.079352 Tokens per Sec: 846.521510\n",
            "Epoch Step: 9701 Loss: 4.516916 Tokens per Sec: 818.082365\n",
            "Epoch Step: 9751 Loss: 4.755733 Tokens per Sec: 829.673532\n",
            "Epoch Step: 9801 Loss: 4.864927 Tokens per Sec: 859.753126\n",
            "Epoch Step: 9851 Loss: 4.760466 Tokens per Sec: 773.304541\n",
            "Epoch Step: 9901 Loss: 4.627775 Tokens per Sec: 813.148845\n",
            "Epoch Step: 9951 Loss: 4.881938 Tokens per Sec: 837.284684\n",
            "Epoch Step: 10001 Loss: 4.178560 Tokens per Sec: 841.377682\n",
            "Epoch Step: 10051 Loss: 4.277300 Tokens per Sec: 823.500595\n",
            "Epoch Step: 10101 Loss: 4.448426 Tokens per Sec: 828.719644\n",
            "Epoch Step: 10151 Loss: 4.694961 Tokens per Sec: 789.330731\n",
            "Epoch Step: 10201 Loss: 4.409935 Tokens per Sec: 808.713864\n",
            "Epoch Step: 10251 Loss: 4.114082 Tokens per Sec: 837.513755\n",
            "Epoch Step: 10301 Loss: 4.370194 Tokens per Sec: 842.608315\n",
            "Epoch Step: 10351 Loss: 4.303994 Tokens per Sec: 843.298642\n",
            "Epoch Step: 10401 Loss: 4.804541 Tokens per Sec: 817.020261\n",
            "Epoch Step: 10451 Loss: 4.621857 Tokens per Sec: 814.093803\n",
            "Epoch Step: 10501 Loss: 4.479012 Tokens per Sec: 835.181279\n",
            "Epoch Step: 10551 Loss: 4.542903 Tokens per Sec: 830.335532\n",
            "Epoch Step: 10601 Loss: 4.149627 Tokens per Sec: 843.156161\n",
            "Epoch Step: 10651 Loss: 4.523819 Tokens per Sec: 844.728562\n",
            "Epoch Step: 10701 Loss: 4.637801 Tokens per Sec: 864.027018\n",
            "Epoch Step: 10751 Loss: 4.455955 Tokens per Sec: 844.383057\n",
            "Epoch Step: 10801 Loss: 4.232007 Tokens per Sec: 840.018275\n",
            "Epoch Step: 10851 Loss: 4.250934 Tokens per Sec: 833.107328\n",
            "Epoch Step: 10901 Loss: 4.468087 Tokens per Sec: 849.279349\n",
            "Epoch Step: 10951 Loss: 4.202480 Tokens per Sec: 831.987158\n",
            "Epoch Step: 11001 Loss: 4.270496 Tokens per Sec: 875.325451\n",
            "Epoch Step: 11051 Loss: 4.587986 Tokens per Sec: 867.285696\n",
            "Epoch Step: 11101 Loss: 4.767257 Tokens per Sec: 820.200567\n",
            "Epoch Step: 11151 Loss: 4.550876 Tokens per Sec: 818.404212\n",
            "Epoch Step: 11201 Loss: 4.028312 Tokens per Sec: 811.043557\n",
            "Epoch Step: 11251 Loss: 4.298779 Tokens per Sec: 804.006655\n",
            "Epoch Step: 11301 Loss: 3.431801 Tokens per Sec: 837.018418\n",
            "Epoch Step: 11351 Loss: 4.119959 Tokens per Sec: 836.847222\n",
            "Epoch Step: 11401 Loss: 4.786234 Tokens per Sec: 829.147933\n",
            "Epoch Step: 11451 Loss: 4.391587 Tokens per Sec: 823.191769\n",
            "Epoch Step: 11501 Loss: 4.762203 Tokens per Sec: 803.292407\n",
            "Epoch Step: 11551 Loss: 4.363335 Tokens per Sec: 822.517093\n",
            "Epoch Step: 11601 Loss: 4.537360 Tokens per Sec: 821.914686\n",
            "Epoch Step: 11651 Loss: 4.477039 Tokens per Sec: 848.266210\n",
            "Epoch Step: 11701 Loss: 4.372056 Tokens per Sec: 813.169522\n",
            "Epoch Step: 11751 Loss: 4.688724 Tokens per Sec: 829.717743\n",
            "Epoch Step: 11801 Loss: 3.997894 Tokens per Sec: 811.869037\n",
            "Epoch Step: 11851 Loss: 5.251830 Tokens per Sec: 863.823520\n",
            "Epoch Step: 11901 Loss: 4.898793 Tokens per Sec: 840.423695\n",
            "Epoch Step: 11951 Loss: 4.281029 Tokens per Sec: 808.718479\n",
            "Epoch Step: 12001 Loss: 3.967878 Tokens per Sec: 825.421331\n",
            "Epoch Step: 12051 Loss: 1.419882 Tokens per Sec: 854.883819\n",
            "Epoch Step: 12101 Loss: 4.473274 Tokens per Sec: 835.771823\n",
            "Epoch Step: 12151 Loss: 4.265918 Tokens per Sec: 824.736986\n",
            "Epoch Step: 12201 Loss: 4.330934 Tokens per Sec: 800.007347\n",
            "Epoch Step: 12251 Loss: 4.426414 Tokens per Sec: 835.328027\n",
            "Epoch Step: 12301 Loss: 4.347433 Tokens per Sec: 842.051502\n",
            "Epoch Step: 12351 Loss: 4.414168 Tokens per Sec: 831.442963\n",
            "Epoch Step: 12401 Loss: 4.388946 Tokens per Sec: 802.212981\n",
            "Epoch Step: 12451 Loss: 4.065300 Tokens per Sec: 840.895738\n",
            "Epoch Step: 12501 Loss: 4.383462 Tokens per Sec: 789.422650\n",
            "Epoch Step: 12551 Loss: 4.529773 Tokens per Sec: 860.508028\n",
            "Epoch Step: 12601 Loss: 4.493078 Tokens per Sec: 843.126839\n",
            "Epoch Step: 12651 Loss: 4.791284 Tokens per Sec: 837.163596\n",
            "Epoch Step: 12701 Loss: 3.765400 Tokens per Sec: 850.466458\n",
            "Epoch Step: 12751 Loss: 4.188954 Tokens per Sec: 830.317942\n",
            "Epoch Step: 12801 Loss: 4.378379 Tokens per Sec: 824.479731\n",
            "Epoch Step: 12851 Loss: 1.835845 Tokens per Sec: 832.533002\n",
            "Epoch Step: 12901 Loss: 4.169674 Tokens per Sec: 808.128005\n",
            "Epoch Step: 12951 Loss: 4.548836 Tokens per Sec: 793.187633\n",
            "Epoch Step: 13001 Loss: 1.915529 Tokens per Sec: 854.586285\n",
            "Epoch Step: 13051 Loss: 4.663867 Tokens per Sec: 811.895202\n",
            "Epoch Step: 13101 Loss: 4.520618 Tokens per Sec: 808.020197\n",
            "Epoch Step: 13151 Loss: 4.342372 Tokens per Sec: 821.318787\n",
            "Epoch Step: 13201 Loss: 4.694036 Tokens per Sec: 822.895085\n",
            "Epoch Step: 13251 Loss: 4.146907 Tokens per Sec: 843.295773\n",
            "Epoch Step: 13301 Loss: 4.732198 Tokens per Sec: 803.472104\n",
            "Epoch Step: 13351 Loss: 4.575093 Tokens per Sec: 815.875847\n",
            "Epoch Step: 13401 Loss: 4.345694 Tokens per Sec: 814.064962\n",
            "Epoch Step: 13451 Loss: 4.783994 Tokens per Sec: 825.002094\n",
            "Epoch Step: 13501 Loss: 4.564109 Tokens per Sec: 857.314866\n",
            "Epoch Step: 13551 Loss: 4.044509 Tokens per Sec: 870.470950\n",
            "Epoch Step: 13601 Loss: 4.546840 Tokens per Sec: 844.914826\n",
            "Epoch Step: 13651 Loss: 4.447632 Tokens per Sec: 834.152797\n",
            "Epoch Step: 13701 Loss: 4.354734 Tokens per Sec: 804.892587\n",
            "Epoch Step: 13751 Loss: 3.963142 Tokens per Sec: 800.152144\n",
            "Epoch Step: 13801 Loss: 4.312483 Tokens per Sec: 820.305488\n",
            "Epoch Step: 13851 Loss: 4.597492 Tokens per Sec: 819.757914\n",
            "Epoch Step: 13901 Loss: 4.386539 Tokens per Sec: 825.818170\n",
            "Epoch Step: 13951 Loss: 4.024750 Tokens per Sec: 848.777199\n",
            "Epoch Step: 14001 Loss: 4.173518 Tokens per Sec: 836.081595\n",
            "Epoch Step: 14051 Loss: 4.299968 Tokens per Sec: 813.521568\n",
            "Epoch Step: 14101 Loss: 4.535714 Tokens per Sec: 832.661182\n",
            "Epoch Step: 14151 Loss: 4.634139 Tokens per Sec: 822.862696\n",
            "Epoch Step: 14201 Loss: 4.401449 Tokens per Sec: 816.491630\n",
            "Epoch Step: 14251 Loss: 4.826684 Tokens per Sec: 801.844839\n",
            "Epoch Step: 14301 Loss: 4.519166 Tokens per Sec: 822.676690\n",
            "Epoch Step: 14351 Loss: 4.714950 Tokens per Sec: 841.558792\n",
            "Epoch Step: 14401 Loss: 4.616172 Tokens per Sec: 838.556936\n",
            "Epoch Step: 14451 Loss: 4.032732 Tokens per Sec: 779.230693\n",
            "Epoch Step: 14501 Loss: 4.435896 Tokens per Sec: 806.308270\n",
            "Epoch Step: 14551 Loss: 4.333497 Tokens per Sec: 827.686591\n",
            "Epoch Step: 14601 Loss: 4.186996 Tokens per Sec: 828.696518\n",
            "Epoch Step: 14651 Loss: 4.158185 Tokens per Sec: 803.787901\n",
            "Epoch Step: 14701 Loss: 4.599188 Tokens per Sec: 805.340011\n",
            "Epoch Step: 14751 Loss: 4.266762 Tokens per Sec: 835.161189\n",
            "Epoch Step: 14801 Loss: 3.908585 Tokens per Sec: 838.934624\n",
            "Epoch Step: 14851 Loss: 4.461213 Tokens per Sec: 837.963967\n",
            "Epoch Step: 14901 Loss: 4.261205 Tokens per Sec: 781.185089\n",
            "Epoch Step: 14951 Loss: 4.771873 Tokens per Sec: 826.584050\n",
            "Epoch Step: 15001 Loss: 4.566363 Tokens per Sec: 800.571685\n",
            "Epoch Step: 15051 Loss: 4.104798 Tokens per Sec: 824.614080\n",
            "Epoch Step: 1 Loss: 0.972923 Tokens per Sec: 535.603123\n",
            "Epoch Step: 51 Loss: 1.552584 Tokens per Sec: 2367.117643\n",
            "Epoch Step: 101 Loss: 1.885483 Tokens per Sec: 2386.273843\n",
            "Epoch Step: 151 Loss: 1.272049 Tokens per Sec: 2386.320743\n",
            "Epoch Step: 201 Loss: 2.500407 Tokens per Sec: 2464.745239\n",
            "Epoch Step: 251 Loss: 1.117630 Tokens per Sec: 2701.938182\n",
            "Epoch Step: 301 Loss: 3.058913 Tokens per Sec: 2778.892022\n",
            "Epoch Step: 351 Loss: 1.425195 Tokens per Sec: 2901.213891\n",
            "Epoch Step: 401 Loss: 3.704605 Tokens per Sec: 2921.109048\n",
            "Epoch Step: 451 Loss: 1.030629 Tokens per Sec: 2828.233768\n",
            "Epoch Step: 501 Loss: 3.792664 Tokens per Sec: 2984.353086\n",
            "Epoch Step: 551 Loss: 3.568840 Tokens per Sec: 2765.346059\n",
            "Epoch Step: 601 Loss: 2.753219 Tokens per Sec: 3048.226023\n",
            "Epoch Step: 651 Loss: 3.891276 Tokens per Sec: 2949.979623\n",
            "Epoch Step: 701 Loss: 3.769036 Tokens per Sec: 3049.137045\n",
            "Epoch Step: 751 Loss: 4.010947 Tokens per Sec: 3148.848500\n",
            "Epoch Step: 801 Loss: 3.505936 Tokens per Sec: 3008.558691\n",
            "Epoch Step: 851 Loss: 4.343912 Tokens per Sec: 3154.446193\n",
            "Epoch Step: 901 Loss: 4.188458 Tokens per Sec: 2951.912105\n",
            "Epoch Step: 951 Loss: 4.068527 Tokens per Sec: 3152.837377\n",
            "Epoch Step: 1001 Loss: 3.886685 Tokens per Sec: 3009.867701\n",
            "Epoch Step: 1051 Loss: 3.746673 Tokens per Sec: 2957.521431\n",
            "Epoch Step: 1101 Loss: 4.177830 Tokens per Sec: 3110.994622\n",
            "Epoch Step: 1151 Loss: 3.776966 Tokens per Sec: 3120.967824\n",
            "Epoch Step: 1201 Loss: 3.868059 Tokens per Sec: 3173.986929\n",
            "Epoch Step: 1251 Loss: 4.000138 Tokens per Sec: 2929.883346\n",
            "Epoch Step: 1301 Loss: 3.912259 Tokens per Sec: 3121.390428\n",
            "Epoch Step: 1351 Loss: 4.206552 Tokens per Sec: 3153.748837\n",
            "Epoch Step: 1401 Loss: 4.487993 Tokens per Sec: 3022.831068\n",
            "Epoch Step: 1451 Loss: 4.236994 Tokens per Sec: 3000.030218\n",
            "Epoch Step: 1501 Loss: 4.407378 Tokens per Sec: 3189.013973\n",
            "Epoch Step: 1551 Loss: 3.765594 Tokens per Sec: 3172.896428\n",
            "Epoch Step: 1601 Loss: 4.068738 Tokens per Sec: 3102.075166\n",
            "Epoch Step: 1651 Loss: 4.108171 Tokens per Sec: 2865.201902\n",
            "Epoch Step: 1701 Loss: 3.723071 Tokens per Sec: 3211.253834\n",
            "Epoch Step: 1751 Loss: 3.942633 Tokens per Sec: 3228.286507\n",
            "Epoch Step: 1801 Loss: 4.126514 Tokens per Sec: 3117.652389\n",
            "Epoch Step: 1851 Loss: 4.145224 Tokens per Sec: 2970.756999\n",
            "Epoch Step: 1901 Loss: 4.230767 Tokens per Sec: 3088.139559\n",
            "Epoch Step: 1951 Loss: 4.089430 Tokens per Sec: 3167.673844\n",
            "Epoch Step: 2001 Loss: 3.845583 Tokens per Sec: 3126.296153\n",
            "Epoch Step: 2051 Loss: 4.175187 Tokens per Sec: 3143.731844\n",
            "Epoch Step: 2101 Loss: 4.001207 Tokens per Sec: 2912.876057\n",
            "Epoch Step: 2151 Loss: 4.274994 Tokens per Sec: 3062.705381\n",
            "Epoch Step: 2201 Loss: 3.920124 Tokens per Sec: 3158.077002\n",
            "Epoch Step: 2251 Loss: 4.160050 Tokens per Sec: 3159.363735\n",
            "Epoch Step: 2301 Loss: 4.005723 Tokens per Sec: 3209.772818\n",
            "Epoch Step: 2351 Loss: 4.230810 Tokens per Sec: 2961.849868\n",
            "Epoch Step: 2401 Loss: 4.444165 Tokens per Sec: 2972.456479\n",
            "Epoch Step: 2451 Loss: 4.319920 Tokens per Sec: 3097.377284\n",
            "Epoch Step: 2501 Loss: 4.338866 Tokens per Sec: 3240.773102\n",
            "Epoch Step: 2551 Loss: 4.236896 Tokens per Sec: 3158.555118\n",
            "Epoch Step: 2601 Loss: 4.078818 Tokens per Sec: 3191.816651\n",
            "Epoch Step: 2651 Loss: 3.905912 Tokens per Sec: 2659.861732\n",
            "Epoch Step: 2701 Loss: 4.291018 Tokens per Sec: 3117.747777\n",
            "Epoch Step: 2751 Loss: 3.908233 Tokens per Sec: 3236.140481\n",
            "Epoch Step: 2801 Loss: 4.627042 Tokens per Sec: 3212.649241\n",
            "Epoch Step: 2851 Loss: 3.937241 Tokens per Sec: 3218.221124\n",
            "Epoch Step: 2901 Loss: 4.333089 Tokens per Sec: 2909.211022\n",
            "Epoch Step: 2951 Loss: 3.638742 Tokens per Sec: 3144.662176\n",
            "Epoch Step: 3001 Loss: 4.034410 Tokens per Sec: 3199.540299\n",
            "Epoch Step: 3051 Loss: 4.055910 Tokens per Sec: 3304.231836\n",
            "Epoch Step: 3101 Loss: 4.313834 Tokens per Sec: 3245.281239\n",
            "Epoch Step: 3151 Loss: 4.862818 Tokens per Sec: 3052.878791\n",
            "Epoch Step: 3201 Loss: 3.834556 Tokens per Sec: 2842.812558\n",
            "Epoch Step: 3251 Loss: 4.528661 Tokens per Sec: 3158.008343\n",
            "Epoch Step: 3301 Loss: 4.111264 Tokens per Sec: 3317.578851\n",
            "Epoch Step: 3351 Loss: 4.058077 Tokens per Sec: 3247.241865\n",
            "Epoch Step: 3401 Loss: 4.245309 Tokens per Sec: 3118.826903\n",
            "Epoch Step: 3451 Loss: 4.055991 Tokens per Sec: 2856.615495\n",
            "Epoch Step: 3501 Loss: 4.491452 Tokens per Sec: 3155.211016\n",
            "Epoch Step: 3551 Loss: 4.228668 Tokens per Sec: 3251.688288\n",
            "Epoch Step: 3601 Loss: 4.027422 Tokens per Sec: 3265.860188\n",
            "Epoch Step: 3651 Loss: 4.024588 Tokens per Sec: 3125.191750\n",
            "Epoch Step: 3701 Loss: 4.314852 Tokens per Sec: 3153.012224\n",
            "Epoch Step: 3751 Loss: 4.228009 Tokens per Sec: 2621.379627\n",
            "Epoch Step: 3801 Loss: 4.263502 Tokens per Sec: 3057.857754\n",
            "Epoch Step: 3851 Loss: 4.532262 Tokens per Sec: 3245.248013\n",
            "Epoch Step: 3901 Loss: 4.125211 Tokens per Sec: 3098.021439\n",
            "Epoch Step: 3951 Loss: 3.963616 Tokens per Sec: 3256.949793\n",
            "Epoch Step: 4001 Loss: 4.511737 Tokens per Sec: 2924.802147\n",
            "Epoch Step: 4051 Loss: 4.487766 Tokens per Sec: 2978.319622\n",
            "Epoch Step: 4101 Loss: 4.298359 Tokens per Sec: 3240.655826\n",
            "Epoch Step: 4151 Loss: 4.333251 Tokens per Sec: 3141.550609\n",
            "Epoch Step: 4201 Loss: 4.282863 Tokens per Sec: 3183.440637\n",
            "Epoch Step: 4251 Loss: 3.729693 Tokens per Sec: 3134.468219\n",
            "Epoch Step: 4301 Loss: 4.030876 Tokens per Sec: 2547.049376\n",
            "Epoch Step: 4351 Loss: 4.445301 Tokens per Sec: 2831.039722\n",
            "Epoch Step: 4401 Loss: 4.714266 Tokens per Sec: 3070.144786\n",
            "Epoch Step: 4451 Loss: 4.289091 Tokens per Sec: 3267.681924\n",
            "Epoch Step: 4501 Loss: 4.438015 Tokens per Sec: 3152.751957\n",
            "Epoch Step: 4551 Loss: 4.028455 Tokens per Sec: 3178.739403\n",
            "Epoch Step: 4601 Loss: 4.301311 Tokens per Sec: 2719.063933\n",
            "Epoch Step: 4651 Loss: 4.491930 Tokens per Sec: 3034.834150\n",
            "Epoch Step: 4701 Loss: 4.333187 Tokens per Sec: 3265.883485\n",
            "Epoch Step: 4751 Loss: 4.220309 Tokens per Sec: 3101.798805\n",
            "Epoch Step: 4801 Loss: 3.925637 Tokens per Sec: 3142.246248\n",
            "Epoch Step: 4851 Loss: 4.402194 Tokens per Sec: 2836.463588\n",
            "Epoch Step: 4901 Loss: 4.303854 Tokens per Sec: 3079.476134\n",
            "Epoch Step: 4951 Loss: 4.290691 Tokens per Sec: 3259.351875\n",
            "Epoch Step: 5001 Loss: 4.377793 Tokens per Sec: 3162.596172\n",
            "Epoch Step: 5051 Loss: 4.439738 Tokens per Sec: 3134.468751\n",
            "Epoch Step: 5101 Loss: 4.276143 Tokens per Sec: 2819.077793\n",
            "Epoch Step: 5151 Loss: 4.370509 Tokens per Sec: 2803.243282\n",
            "Epoch Step: 5201 Loss: 4.325811 Tokens per Sec: 3050.522441\n",
            "Epoch Step: 5251 Loss: 4.504067 Tokens per Sec: 3269.739640\n",
            "Epoch Step: 5301 Loss: 4.191435 Tokens per Sec: 3065.279398\n",
            "Epoch Step: 5351 Loss: 4.080417 Tokens per Sec: 2943.408510\n",
            "Epoch Step: 5401 Loss: 4.201209 Tokens per Sec: 2795.241839\n",
            "Epoch Step: 5451 Loss: 4.058830 Tokens per Sec: 3105.432832\n",
            "Epoch Step: 5501 Loss: 4.039099 Tokens per Sec: 3148.721790\n",
            "Epoch Step: 5551 Loss: 4.314704 Tokens per Sec: 3074.459589\n",
            "Epoch Step: 5601 Loss: 4.570329 Tokens per Sec: 3045.413838\n",
            "Epoch Step: 5651 Loss: 4.659379 Tokens per Sec: 2901.820819\n",
            "Epoch Step: 5701 Loss: 4.542759 Tokens per Sec: 3268.371912\n",
            "Epoch Step: 5751 Loss: 4.044895 Tokens per Sec: 3113.838629\n",
            "Epoch Step: 5801 Loss: 4.222387 Tokens per Sec: 3236.489305\n",
            "Epoch Step: 5851 Loss: 4.734010 Tokens per Sec: 2586.843248\n",
            "Epoch Step: 5901 Loss: 4.607286 Tokens per Sec: 2662.748726\n",
            "Epoch Step: 5951 Loss: 4.318160 Tokens per Sec: 2924.456579\n",
            "Epoch Step: 6001 Loss: 3.961706 Tokens per Sec: 3147.779018\n",
            "Epoch Step: 6051 Loss: 4.154652 Tokens per Sec: 3206.547436\n",
            "Epoch Step: 6101 Loss: 4.597905 Tokens per Sec: 2633.166732\n",
            "Epoch Step: 6151 Loss: 4.279912 Tokens per Sec: 2842.559027\n",
            "Epoch Step: 6201 Loss: 4.147940 Tokens per Sec: 3080.478529\n",
            "Epoch Step: 6251 Loss: 4.155569 Tokens per Sec: 3229.956637\n",
            "Epoch Step: 6301 Loss: 4.585680 Tokens per Sec: 2857.238827\n",
            "Epoch Step: 6351 Loss: 4.509667 Tokens per Sec: 2749.380752\n",
            "Epoch Step: 6401 Loss: 4.324894 Tokens per Sec: 3077.785688\n",
            "Epoch Step: 6451 Loss: 4.163301 Tokens per Sec: 3201.819835\n",
            "Epoch Step: 6501 Loss: 4.792287 Tokens per Sec: 3061.468281\n",
            "Epoch Step: 6551 Loss: 4.123719 Tokens per Sec: 2727.639446\n",
            "Epoch Step: 6601 Loss: 4.159039 Tokens per Sec: 3179.471824\n",
            "Epoch Step: 6651 Loss: 4.394316 Tokens per Sec: 3120.450142\n",
            "Epoch Step: 6701 Loss: 4.355102 Tokens per Sec: 3090.084165\n",
            "Epoch Step: 6751 Loss: 4.610107 Tokens per Sec: 2769.045483\n",
            "Epoch Step: 6801 Loss: 4.464532 Tokens per Sec: 3272.914056\n",
            "Epoch Step: 6851 Loss: 3.867819 Tokens per Sec: 3030.849539\n",
            "Epoch Step: 6901 Loss: 4.345465 Tokens per Sec: 3047.347456\n",
            "Epoch Step: 6951 Loss: 4.263650 Tokens per Sec: 2476.298207\n",
            "Epoch Step: 7001 Loss: 4.367381 Tokens per Sec: 2760.825044\n",
            "Epoch Step: 7051 Loss: 4.301802 Tokens per Sec: 2976.930617\n",
            "Epoch Step: 7101 Loss: 4.507744 Tokens per Sec: 3093.440462\n",
            "Epoch Step: 7151 Loss: 4.448130 Tokens per Sec: 2458.794778\n",
            "Epoch Step: 7201 Loss: 4.192383 Tokens per Sec: 2889.943996\n",
            "Epoch Step: 7251 Loss: 4.354169 Tokens per Sec: 3233.687263\n",
            "Epoch Step: 7301 Loss: 4.570353 Tokens per Sec: 2676.689017\n",
            "Epoch Step: 7351 Loss: 3.975630 Tokens per Sec: 2878.784396\n",
            "Epoch Step: 7401 Loss: 4.311471 Tokens per Sec: 3218.057228\n",
            "Epoch Step: 7451 Loss: 4.528321 Tokens per Sec: 2875.640149\n",
            "Epoch Step: 7501 Loss: 4.412488 Tokens per Sec: 2826.222464\n",
            "Epoch Step: 7551 Loss: 4.641419 Tokens per Sec: 3155.752876\n",
            "Epoch Step: 7601 Loss: 4.128065 Tokens per Sec: 2977.197457\n",
            "Epoch Step: 7651 Loss: 4.285704 Tokens per Sec: 2781.254941\n",
            "Epoch Step: 7701 Loss: 4.177769 Tokens per Sec: 3225.453704\n",
            "Epoch Step: 7751 Loss: 4.084267 Tokens per Sec: 3014.588569\n",
            "Epoch Step: 7801 Loss: 4.471396 Tokens per Sec: 2901.769802\n",
            "Epoch Step: 7851 Loss: 4.238858 Tokens per Sec: 3153.439081\n",
            "Epoch Step: 7901 Loss: 4.638453 Tokens per Sec: 2868.072429\n",
            "Epoch Step: 7951 Loss: 4.625589 Tokens per Sec: 3164.537837\n",
            "Epoch Step: 8001 Loss: 4.103234 Tokens per Sec: 2998.109240\n",
            "Epoch Step: 8051 Loss: 4.839954 Tokens per Sec: 2540.810227\n",
            "Epoch Step: 8101 Loss: 4.292973 Tokens per Sec: 2707.399993\n",
            "Epoch Step: 8151 Loss: 4.589151 Tokens per Sec: 3005.241173\n",
            "Epoch Step: 8201 Loss: 4.356408 Tokens per Sec: 2463.527465\n",
            "Epoch Step: 8251 Loss: 4.628013 Tokens per Sec: 2741.863807\n",
            "Epoch Step: 8301 Loss: 4.503584 Tokens per Sec: 3032.150219\n",
            "Epoch Step: 8351 Loss: 4.583096 Tokens per Sec: 2520.597933\n",
            "Epoch Step: 8401 Loss: 4.242786 Tokens per Sec: 2949.938941\n",
            "Epoch Step: 8451 Loss: 4.437294 Tokens per Sec: 2815.002906\n",
            "Epoch Step: 8501 Loss: 4.364479 Tokens per Sec: 2862.890952\n",
            "Epoch Step: 8551 Loss: 4.306947 Tokens per Sec: 3122.714281\n",
            "Epoch Step: 8601 Loss: 4.540763 Tokens per Sec: 2657.166012\n",
            "Epoch Step: 8651 Loss: 4.184804 Tokens per Sec: 3107.543671\n",
            "Epoch Step: 8701 Loss: 4.479718 Tokens per Sec: 2773.405137\n",
            "Epoch Step: 8751 Loss: 4.135386 Tokens per Sec: 3095.393708\n",
            "Epoch Step: 8801 Loss: 4.348289 Tokens per Sec: 2883.691383\n",
            "Epoch Step: 8851 Loss: 4.656465 Tokens per Sec: 3215.913592\n",
            "Epoch Step: 8901 Loss: 4.568479 Tokens per Sec: 2804.775301\n",
            "Epoch Step: 8951 Loss: 4.288647 Tokens per Sec: 3138.796028\n",
            "Epoch Step: 9001 Loss: 4.066041 Tokens per Sec: 2714.151978\n",
            "Epoch Step: 9051 Loss: 4.746532 Tokens per Sec: 3112.281870\n",
            "Epoch Step: 9101 Loss: 4.638340 Tokens per Sec: 2833.957393\n",
            "Epoch Step: 9151 Loss: 4.575437 Tokens per Sec: 2965.128555\n",
            "Epoch Step: 9201 Loss: 4.321329 Tokens per Sec: 2499.383489\n",
            "Epoch Step: 9251 Loss: 4.660912 Tokens per Sec: 2557.436626\n",
            "Epoch Step: 9301 Loss: 4.417302 Tokens per Sec: 2622.235462\n",
            "Epoch Step: 9351 Loss: 4.423113 Tokens per Sec: 2577.350726\n",
            "Epoch Step: 9401 Loss: 4.666726 Tokens per Sec: 2637.307106\n",
            "Epoch Step: 9451 Loss: 4.198138 Tokens per Sec: 2649.764507\n",
            "Epoch Step: 9501 Loss: 4.663405 Tokens per Sec: 2759.738490\n",
            "Epoch Step: 9551 Loss: 4.473902 Tokens per Sec: 2671.676995\n",
            "Epoch Step: 9601 Loss: 4.833328 Tokens per Sec: 2706.750863\n",
            "Epoch Step: 9651 Loss: 4.486838 Tokens per Sec: 2740.661110\n",
            "Epoch Step: 9701 Loss: 4.530519 Tokens per Sec: 2785.837253\n",
            "Epoch Step: 9751 Loss: 4.005922 Tokens per Sec: 2892.066320\n",
            "Epoch Step: 9801 Loss: 4.334334 Tokens per Sec: 2600.957503\n",
            "Epoch Step: 9851 Loss: 4.254686 Tokens per Sec: 2926.674446\n",
            "Epoch Step: 9901 Loss: 4.646359 Tokens per Sec: 2720.888384\n",
            "Epoch Step: 9951 Loss: 4.665740 Tokens per Sec: 2811.815300\n",
            "Epoch Step: 10001 Loss: 4.416649 Tokens per Sec: 2981.853370\n",
            "Epoch Step: 10051 Loss: 4.262996 Tokens per Sec: 2695.868399\n",
            "Epoch Step: 10101 Loss: 4.791491 Tokens per Sec: 2756.303083\n",
            "Epoch Step: 10151 Loss: 3.945066 Tokens per Sec: 3073.374374\n",
            "Epoch Step: 10201 Loss: 4.362554 Tokens per Sec: 2642.722850\n",
            "Epoch Step: 10251 Loss: 4.541477 Tokens per Sec: 2847.410837\n",
            "Epoch Step: 10301 Loss: 3.927956 Tokens per Sec: 3010.513175\n",
            "Epoch Step: 10351 Loss: 4.584136 Tokens per Sec: 2760.613657\n",
            "Epoch Step: 10401 Loss: 4.379167 Tokens per Sec: 2774.351692\n",
            "Epoch Step: 10451 Loss: 4.241415 Tokens per Sec: 3004.709562\n",
            "Epoch Step: 10501 Loss: 4.420281 Tokens per Sec: 2610.638453\n",
            "Epoch Step: 10551 Loss: 3.832673 Tokens per Sec: 2711.866704\n",
            "Epoch Step: 10601 Loss: 4.133931 Tokens per Sec: 2110.757059\n",
            "Epoch Step: 10651 Loss: 4.206034 Tokens per Sec: 2370.823699\n",
            "Epoch Step: 10701 Loss: 4.503856 Tokens per Sec: 2131.787980\n",
            "Epoch Step: 10751 Loss: 4.136397 Tokens per Sec: 2260.565021\n",
            "Epoch Step: 10801 Loss: 4.527833 Tokens per Sec: 2308.907338\n",
            "Epoch Step: 10851 Loss: 4.463964 Tokens per Sec: 2304.469278\n",
            "Epoch Step: 10901 Loss: 4.258557 Tokens per Sec: 2431.091258\n",
            "Epoch Step: 10951 Loss: 4.466292 Tokens per Sec: 2379.580218\n",
            "Epoch Step: 11001 Loss: 4.023930 Tokens per Sec: 2552.137348\n",
            "Epoch Step: 11051 Loss: 4.183764 Tokens per Sec: 2385.188921\n",
            "Epoch Step: 11101 Loss: 3.988429 Tokens per Sec: 2557.483567\n",
            "Epoch Step: 11151 Loss: 4.352859 Tokens per Sec: 2560.782401\n",
            "Epoch Step: 11201 Loss: 4.819081 Tokens per Sec: 2384.418358\n",
            "Epoch Step: 11251 Loss: 4.482419 Tokens per Sec: 2502.792875\n",
            "Epoch Step: 11301 Loss: 4.639244 Tokens per Sec: 2577.787826\n",
            "Epoch Step: 11351 Loss: 4.491629 Tokens per Sec: 2492.547293\n",
            "Epoch Step: 11401 Loss: 3.987951 Tokens per Sec: 2598.589121\n",
            "Epoch Step: 11451 Loss: 4.115776 Tokens per Sec: 2751.982444\n",
            "Epoch Step: 11501 Loss: 4.533014 Tokens per Sec: 2421.953213\n",
            "Epoch Step: 11551 Loss: 4.597589 Tokens per Sec: 2622.905841\n",
            "Epoch Step: 11601 Loss: 4.656266 Tokens per Sec: 2699.953479\n",
            "Epoch Step: 11651 Loss: 4.637454 Tokens per Sec: 2574.652692\n",
            "Epoch Step: 11701 Loss: 4.300434 Tokens per Sec: 2639.213139\n",
            "Epoch Step: 11751 Loss: 4.787276 Tokens per Sec: 2740.240806\n",
            "Epoch Step: 11801 Loss: 4.055707 Tokens per Sec: 2721.653119\n",
            "Epoch Step: 11851 Loss: 4.582476 Tokens per Sec: 2678.368557\n",
            "Epoch Step: 11901 Loss: 4.149178 Tokens per Sec: 2892.651923\n",
            "Epoch Step: 11951 Loss: 4.564892 Tokens per Sec: 2731.134092\n",
            "Epoch Step: 12001 Loss: 4.571117 Tokens per Sec: 2907.022106\n",
            "Epoch Step: 12051 Loss: 4.267380 Tokens per Sec: 2802.081345\n",
            "Epoch Step: 12101 Loss: 4.613406 Tokens per Sec: 2878.788112\n",
            "Epoch Step: 12151 Loss: 4.555269 Tokens per Sec: 2881.248653\n",
            "Epoch Step: 12201 Loss: 4.599854 Tokens per Sec: 3012.977092\n",
            "Epoch Step: 12251 Loss: 4.248958 Tokens per Sec: 2988.314868\n",
            "Epoch Step: 12301 Loss: 4.317640 Tokens per Sec: 2823.802811\n",
            "Epoch Step: 12351 Loss: 4.249434 Tokens per Sec: 2845.735899\n",
            "Epoch Step: 12401 Loss: 4.292123 Tokens per Sec: 2735.582590\n",
            "Epoch Step: 12451 Loss: 4.262240 Tokens per Sec: 2618.109324\n",
            "Epoch Step: 12501 Loss: 4.678913 Tokens per Sec: 2730.639076\n",
            "Epoch Step: 12551 Loss: 4.297736 Tokens per Sec: 2754.378613\n",
            "Epoch Step: 12601 Loss: 3.864115 Tokens per Sec: 2513.255786\n",
            "Epoch Step: 12651 Loss: 4.487501 Tokens per Sec: 2617.505909\n",
            "Epoch Step: 12701 Loss: 4.828252 Tokens per Sec: 2268.349336\n",
            "Epoch Step: 12751 Loss: 4.620283 Tokens per Sec: 1778.178907\n",
            "Epoch Step: 12801 Loss: 4.548134 Tokens per Sec: 1690.662738\n",
            "Epoch Step: 12851 Loss: 4.524302 Tokens per Sec: 1786.829185\n",
            "Epoch Step: 12901 Loss: 5.171558 Tokens per Sec: 1796.438618\n",
            "Epoch Step: 12951 Loss: 4.250329 Tokens per Sec: 1765.034721\n",
            "Epoch Step: 13001 Loss: 4.172883 Tokens per Sec: 1777.013123\n",
            "Epoch Step: 13051 Loss: 4.707784 Tokens per Sec: 1781.145212\n",
            "Epoch Step: 13101 Loss: 4.907269 Tokens per Sec: 1803.187836\n",
            "Epoch Step: 13151 Loss: 4.010793 Tokens per Sec: 1820.168816\n",
            "Epoch Step: 13201 Loss: 4.398337 Tokens per Sec: 1794.815473\n",
            "Epoch Step: 13251 Loss: 4.477191 Tokens per Sec: 1855.078583\n",
            "Epoch Step: 13301 Loss: 5.200297 Tokens per Sec: 1832.935606\n",
            "Epoch Step: 13351 Loss: 4.936817 Tokens per Sec: 1842.520446\n",
            "Epoch Step: 13401 Loss: 4.322941 Tokens per Sec: 1899.589539\n",
            "Epoch Step: 13451 Loss: 4.567527 Tokens per Sec: 1927.005304\n",
            "Epoch Step: 13501 Loss: 5.318055 Tokens per Sec: 1973.083337\n",
            "Epoch Step: 13551 Loss: 4.828332 Tokens per Sec: 1896.658078\n",
            "Epoch Step: 13601 Loss: 4.696336 Tokens per Sec: 1960.883920\n",
            "Epoch Step: 13651 Loss: 4.487364 Tokens per Sec: 1945.775185\n",
            "Epoch Step: 13701 Loss: 4.796495 Tokens per Sec: 1943.872292\n",
            "Epoch Step: 13751 Loss: 4.555849 Tokens per Sec: 1994.929931\n",
            "Epoch Step: 13801 Loss: 4.736624 Tokens per Sec: 1995.087698\n",
            "Epoch Step: 13851 Loss: 4.405452 Tokens per Sec: 2029.499082\n",
            "Epoch Step: 13901 Loss: 4.504824 Tokens per Sec: 2013.318340\n",
            "Epoch Step: 13951 Loss: 4.673614 Tokens per Sec: 2111.753820\n",
            "Epoch Step: 14001 Loss: 4.872622 Tokens per Sec: 2138.800127\n",
            "Epoch Step: 14051 Loss: 4.695981 Tokens per Sec: 2167.759242\n",
            "Epoch Step: 14101 Loss: 4.789934 Tokens per Sec: 2178.162622\n",
            "Epoch Step: 14151 Loss: 4.531780 Tokens per Sec: 2143.202142\n",
            "Epoch Step: 14201 Loss: 4.907325 Tokens per Sec: 2223.932955\n",
            "Epoch Step: 14251 Loss: 4.395362 Tokens per Sec: 2188.758759\n",
            "Epoch Step: 14301 Loss: 4.489305 Tokens per Sec: 2311.160406\n",
            "Epoch Step: 14351 Loss: 4.230034 Tokens per Sec: 2284.946500\n",
            "Epoch Step: 14401 Loss: 4.413006 Tokens per Sec: 2341.001333\n",
            "Epoch Step: 14451 Loss: 4.798076 Tokens per Sec: 2205.345771\n",
            "Epoch Step: 14501 Loss: 4.811610 Tokens per Sec: 2392.339167\n",
            "Epoch Step: 14551 Loss: 4.693216 Tokens per Sec: 2447.472920\n",
            "Epoch Step: 14601 Loss: 4.348149 Tokens per Sec: 2478.567190\n",
            "Epoch Step: 14651 Loss: 4.341425 Tokens per Sec: 2540.751600\n",
            "Epoch Step: 14701 Loss: 3.770584 Tokens per Sec: 2623.890007\n",
            "Epoch Step: 14751 Loss: 4.497727 Tokens per Sec: 2679.879664\n",
            "Epoch Step: 14801 Loss: 4.312353 Tokens per Sec: 2804.737801\n",
            "Epoch Step: 14851 Loss: 4.030064 Tokens per Sec: 2958.834000\n",
            "Epoch Step: 14901 Loss: 4.362563 Tokens per Sec: 3014.087652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4.248995650763847\n",
            "Epoch Step: 1 Loss: 4.041449 Tokens per Sec: 557.220869\n",
            "Epoch Step: 51 Loss: 4.208298 Tokens per Sec: 836.830510\n",
            "Epoch Step: 101 Loss: 4.745746 Tokens per Sec: 790.183554\n",
            "Epoch Step: 151 Loss: 4.860600 Tokens per Sec: 825.364866\n",
            "Epoch Step: 201 Loss: 4.547198 Tokens per Sec: 808.717123\n",
            "Epoch Step: 251 Loss: 3.838269 Tokens per Sec: 859.149894\n",
            "Epoch Step: 301 Loss: 4.334904 Tokens per Sec: 834.735470\n",
            "Epoch Step: 351 Loss: 4.124991 Tokens per Sec: 826.010069\n",
            "Epoch Step: 401 Loss: 4.363365 Tokens per Sec: 843.630218\n",
            "Epoch Step: 451 Loss: 4.608415 Tokens per Sec: 824.147528\n",
            "Epoch Step: 501 Loss: 4.084651 Tokens per Sec: 864.336104\n",
            "Epoch Step: 551 Loss: 4.147225 Tokens per Sec: 803.017968\n",
            "Epoch Step: 601 Loss: 4.245602 Tokens per Sec: 854.385988\n",
            "Epoch Step: 651 Loss: 4.278148 Tokens per Sec: 854.868915\n",
            "Epoch Step: 701 Loss: 4.722028 Tokens per Sec: 846.474086\n",
            "Epoch Step: 751 Loss: 4.611510 Tokens per Sec: 808.598189\n",
            "Epoch Step: 801 Loss: 4.283423 Tokens per Sec: 866.519027\n",
            "Epoch Step: 851 Loss: 4.629580 Tokens per Sec: 784.054996\n",
            "Epoch Step: 901 Loss: 4.298359 Tokens per Sec: 840.677184\n",
            "Epoch Step: 951 Loss: 4.740939 Tokens per Sec: 829.838737\n",
            "Epoch Step: 1001 Loss: 4.048893 Tokens per Sec: 805.055753\n",
            "Epoch Step: 1051 Loss: 4.548782 Tokens per Sec: 830.293792\n",
            "Epoch Step: 1101 Loss: 4.185002 Tokens per Sec: 857.712984\n",
            "Epoch Step: 1151 Loss: 4.421391 Tokens per Sec: 829.558771\n",
            "Epoch Step: 1201 Loss: 4.102725 Tokens per Sec: 830.579594\n",
            "Epoch Step: 1251 Loss: 4.122063 Tokens per Sec: 838.848093\n",
            "Epoch Step: 1301 Loss: 4.347713 Tokens per Sec: 824.809268\n",
            "Epoch Step: 1351 Loss: 4.321957 Tokens per Sec: 847.026628\n",
            "Epoch Step: 1401 Loss: 4.407261 Tokens per Sec: 789.511413\n",
            "Epoch Step: 1451 Loss: 4.364906 Tokens per Sec: 864.361690\n",
            "Epoch Step: 1501 Loss: 4.308521 Tokens per Sec: 833.531537\n",
            "Epoch Step: 1551 Loss: 3.943969 Tokens per Sec: 819.873674\n",
            "Epoch Step: 1601 Loss: 4.384476 Tokens per Sec: 824.079724\n",
            "Epoch Step: 1651 Loss: 4.446589 Tokens per Sec: 821.896101\n",
            "Epoch Step: 1701 Loss: 4.157627 Tokens per Sec: 802.009939\n",
            "Epoch Step: 1751 Loss: 3.808539 Tokens per Sec: 866.147172\n",
            "Epoch Step: 1801 Loss: 4.344467 Tokens per Sec: 799.351135\n",
            "Epoch Step: 1851 Loss: 4.641026 Tokens per Sec: 815.275260\n",
            "Epoch Step: 1901 Loss: 4.472069 Tokens per Sec: 847.513947\n",
            "Epoch Step: 1951 Loss: 4.506581 Tokens per Sec: 841.423539\n",
            "Epoch Step: 2001 Loss: 4.511017 Tokens per Sec: 833.755749\n",
            "Epoch Step: 2051 Loss: 4.104323 Tokens per Sec: 836.832374\n",
            "Epoch Step: 2101 Loss: 4.314865 Tokens per Sec: 811.401925\n",
            "Epoch Step: 2151 Loss: 4.494305 Tokens per Sec: 863.783736\n",
            "Epoch Step: 2201 Loss: 4.633713 Tokens per Sec: 850.126239\n",
            "Epoch Step: 2251 Loss: 4.479681 Tokens per Sec: 858.519701\n",
            "Epoch Step: 2301 Loss: 4.505107 Tokens per Sec: 811.984136\n",
            "Epoch Step: 2351 Loss: 4.723181 Tokens per Sec: 796.789956\n",
            "Epoch Step: 2401 Loss: 4.319186 Tokens per Sec: 841.352112\n",
            "Epoch Step: 2451 Loss: 4.348359 Tokens per Sec: 839.011527\n",
            "Epoch Step: 2501 Loss: 4.217211 Tokens per Sec: 824.404013\n",
            "Epoch Step: 2551 Loss: 4.313365 Tokens per Sec: 841.202492\n",
            "Epoch Step: 2601 Loss: 4.209777 Tokens per Sec: 826.356575\n",
            "Epoch Step: 2651 Loss: 4.842860 Tokens per Sec: 821.026006\n",
            "Epoch Step: 2701 Loss: 4.376723 Tokens per Sec: 833.295189\n",
            "Epoch Step: 2751 Loss: 4.343832 Tokens per Sec: 823.577820\n",
            "Epoch Step: 2801 Loss: 4.445674 Tokens per Sec: 856.909877\n",
            "Epoch Step: 2851 Loss: 4.038859 Tokens per Sec: 861.090996\n",
            "Epoch Step: 2901 Loss: 4.445584 Tokens per Sec: 808.413489\n",
            "Epoch Step: 2951 Loss: 4.187264 Tokens per Sec: 841.044038\n",
            "Epoch Step: 3001 Loss: 4.343515 Tokens per Sec: 813.101794\n",
            "Epoch Step: 3051 Loss: 4.549042 Tokens per Sec: 833.521398\n",
            "Epoch Step: 3101 Loss: 4.332639 Tokens per Sec: 853.431066\n",
            "Epoch Step: 3151 Loss: 4.459694 Tokens per Sec: 854.574524\n",
            "Epoch Step: 3201 Loss: 4.472469 Tokens per Sec: 827.722243\n",
            "Epoch Step: 3251 Loss: 4.780257 Tokens per Sec: 805.025134\n",
            "Epoch Step: 3301 Loss: 4.498437 Tokens per Sec: 801.473749\n",
            "Epoch Step: 3351 Loss: 4.255408 Tokens per Sec: 845.067133\n",
            "Epoch Step: 3401 Loss: 4.491139 Tokens per Sec: 824.043279\n",
            "Epoch Step: 3451 Loss: 4.534242 Tokens per Sec: 820.435131\n",
            "Epoch Step: 3501 Loss: 4.133858 Tokens per Sec: 807.928125\n",
            "Epoch Step: 3551 Loss: 4.335116 Tokens per Sec: 813.025975\n",
            "Epoch Step: 3601 Loss: 4.440447 Tokens per Sec: 835.238693\n",
            "Epoch Step: 3651 Loss: 4.498141 Tokens per Sec: 831.285262\n",
            "Epoch Step: 3701 Loss: 4.534122 Tokens per Sec: 812.214474\n",
            "Epoch Step: 3751 Loss: 4.140831 Tokens per Sec: 820.601971\n",
            "Epoch Step: 3801 Loss: 4.469889 Tokens per Sec: 826.451553\n",
            "Epoch Step: 3851 Loss: 4.184694 Tokens per Sec: 823.127610\n",
            "Epoch Step: 3901 Loss: 4.696068 Tokens per Sec: 831.762538\n",
            "Epoch Step: 3951 Loss: 4.293851 Tokens per Sec: 816.258477\n",
            "Epoch Step: 4001 Loss: 4.545555 Tokens per Sec: 832.929972\n",
            "Epoch Step: 4051 Loss: 4.724412 Tokens per Sec: 844.746951\n",
            "Epoch Step: 4101 Loss: 4.556503 Tokens per Sec: 820.204757\n",
            "Epoch Step: 4151 Loss: 3.953849 Tokens per Sec: 857.428838\n",
            "Epoch Step: 4201 Loss: 4.393441 Tokens per Sec: 825.987968\n",
            "Epoch Step: 4251 Loss: 4.102188 Tokens per Sec: 865.149754\n",
            "Epoch Step: 4301 Loss: 4.363150 Tokens per Sec: 840.089131\n",
            "Epoch Step: 4351 Loss: 4.288338 Tokens per Sec: 868.141918\n",
            "Epoch Step: 4401 Loss: 4.388156 Tokens per Sec: 845.392172\n",
            "Epoch Step: 4451 Loss: 4.036013 Tokens per Sec: 810.174186\n",
            "Epoch Step: 4501 Loss: 4.286799 Tokens per Sec: 843.393822\n",
            "Epoch Step: 4551 Loss: 4.405682 Tokens per Sec: 830.599762\n",
            "Epoch Step: 4601 Loss: 4.117474 Tokens per Sec: 858.542204\n",
            "Epoch Step: 4651 Loss: 4.480793 Tokens per Sec: 794.428307\n",
            "Epoch Step: 4701 Loss: 4.548354 Tokens per Sec: 803.256425\n",
            "Epoch Step: 4751 Loss: 4.163672 Tokens per Sec: 834.061491\n",
            "Epoch Step: 4801 Loss: 4.359858 Tokens per Sec: 857.256903\n",
            "Epoch Step: 4851 Loss: 4.416696 Tokens per Sec: 847.687188\n",
            "Epoch Step: 4901 Loss: 3.981698 Tokens per Sec: 838.012053\n",
            "Epoch Step: 4951 Loss: 3.753421 Tokens per Sec: 836.024388\n",
            "Epoch Step: 5001 Loss: 4.528384 Tokens per Sec: 818.702907\n",
            "Epoch Step: 5051 Loss: 3.923944 Tokens per Sec: 838.923590\n",
            "Epoch Step: 5101 Loss: 3.937929 Tokens per Sec: 830.439889\n",
            "Epoch Step: 5151 Loss: 3.938282 Tokens per Sec: 872.412092\n",
            "Epoch Step: 5201 Loss: 4.479876 Tokens per Sec: 840.373304\n",
            "Epoch Step: 5251 Loss: 3.915868 Tokens per Sec: 830.055921\n",
            "Epoch Step: 5301 Loss: 4.685196 Tokens per Sec: 803.589441\n",
            "Epoch Step: 5351 Loss: 4.589679 Tokens per Sec: 844.506512\n",
            "Epoch Step: 5401 Loss: 4.333558 Tokens per Sec: 870.190734\n",
            "Epoch Step: 5451 Loss: 4.134981 Tokens per Sec: 791.921073\n",
            "Epoch Step: 5501 Loss: 4.385170 Tokens per Sec: 851.203588\n",
            "Epoch Step: 5551 Loss: 4.101169 Tokens per Sec: 865.840981\n",
            "Epoch Step: 5601 Loss: 4.007935 Tokens per Sec: 843.588792\n",
            "Epoch Step: 5651 Loss: 4.149939 Tokens per Sec: 868.362804\n",
            "Epoch Step: 5701 Loss: 4.696476 Tokens per Sec: 855.757941\n",
            "Epoch Step: 5751 Loss: 4.431620 Tokens per Sec: 840.844047\n",
            "Epoch Step: 5801 Loss: 4.528469 Tokens per Sec: 814.575976\n",
            "Epoch Step: 5851 Loss: 4.638156 Tokens per Sec: 845.710667\n",
            "Epoch Step: 5901 Loss: 3.070499 Tokens per Sec: 809.942665\n",
            "Epoch Step: 5951 Loss: 4.487089 Tokens per Sec: 863.036317\n",
            "Epoch Step: 6001 Loss: 4.167337 Tokens per Sec: 816.987597\n",
            "Epoch Step: 6051 Loss: 4.122899 Tokens per Sec: 843.807015\n",
            "Epoch Step: 6101 Loss: 4.454196 Tokens per Sec: 847.114824\n",
            "Epoch Step: 6151 Loss: 4.592667 Tokens per Sec: 850.823499\n",
            "Epoch Step: 6201 Loss: 1.522160 Tokens per Sec: 829.969514\n",
            "Epoch Step: 6251 Loss: 4.574497 Tokens per Sec: 842.094636\n",
            "Epoch Step: 6301 Loss: 4.144951 Tokens per Sec: 844.712612\n",
            "Epoch Step: 6351 Loss: 4.177835 Tokens per Sec: 860.053684\n",
            "Epoch Step: 6401 Loss: 4.033370 Tokens per Sec: 832.161969\n",
            "Epoch Step: 6451 Loss: 4.087799 Tokens per Sec: 814.220541\n",
            "Epoch Step: 6501 Loss: 4.908672 Tokens per Sec: 823.015682\n",
            "Epoch Step: 6551 Loss: 4.271887 Tokens per Sec: 835.864714\n",
            "Epoch Step: 6601 Loss: 2.339385 Tokens per Sec: 830.572701\n",
            "Epoch Step: 6651 Loss: 4.555570 Tokens per Sec: 833.397693\n",
            "Epoch Step: 6701 Loss: 4.515832 Tokens per Sec: 826.193610\n",
            "Epoch Step: 6751 Loss: 4.557535 Tokens per Sec: 846.948850\n",
            "Epoch Step: 6801 Loss: 3.570191 Tokens per Sec: 852.251504\n",
            "Epoch Step: 6851 Loss: 4.154223 Tokens per Sec: 814.809058\n",
            "Epoch Step: 6901 Loss: 4.488737 Tokens per Sec: 839.711663\n",
            "Epoch Step: 6951 Loss: 4.078383 Tokens per Sec: 814.221800\n",
            "Epoch Step: 7001 Loss: 3.831873 Tokens per Sec: 854.035456\n",
            "Epoch Step: 7051 Loss: 4.100530 Tokens per Sec: 868.672545\n",
            "Epoch Step: 7101 Loss: 4.151444 Tokens per Sec: 819.812831\n",
            "Epoch Step: 7151 Loss: 4.035781 Tokens per Sec: 821.314419\n",
            "Epoch Step: 7201 Loss: 4.127113 Tokens per Sec: 856.351195\n",
            "Epoch Step: 7251 Loss: 4.288003 Tokens per Sec: 836.763382\n",
            "Epoch Step: 7301 Loss: 4.649447 Tokens per Sec: 831.779501\n",
            "Epoch Step: 7351 Loss: 4.486049 Tokens per Sec: 831.304912\n",
            "Epoch Step: 7401 Loss: 4.318134 Tokens per Sec: 841.862141\n",
            "Epoch Step: 7451 Loss: 4.352907 Tokens per Sec: 809.984643\n",
            "Epoch Step: 7501 Loss: 4.117450 Tokens per Sec: 818.930616\n",
            "Epoch Step: 7551 Loss: 4.323276 Tokens per Sec: 829.683488\n",
            "Epoch Step: 7601 Loss: 4.047897 Tokens per Sec: 820.861226\n",
            "Epoch Step: 7651 Loss: 4.111279 Tokens per Sec: 835.383279\n",
            "Epoch Step: 7701 Loss: 3.946944 Tokens per Sec: 844.845102\n",
            "Epoch Step: 7751 Loss: 4.561504 Tokens per Sec: 833.907005\n",
            "Epoch Step: 7801 Loss: 3.984449 Tokens per Sec: 870.232809\n",
            "Epoch Step: 7851 Loss: 3.651311 Tokens per Sec: 826.375843\n",
            "Epoch Step: 7901 Loss: 4.213302 Tokens per Sec: 850.881174\n",
            "Epoch Step: 7951 Loss: 5.014320 Tokens per Sec: 832.657173\n",
            "Epoch Step: 8001 Loss: 4.393020 Tokens per Sec: 782.544184\n",
            "Epoch Step: 8051 Loss: 4.467543 Tokens per Sec: 822.896070\n",
            "Epoch Step: 8101 Loss: 4.744404 Tokens per Sec: 809.423522\n",
            "Epoch Step: 8151 Loss: 4.356314 Tokens per Sec: 813.374189\n",
            "Epoch Step: 8201 Loss: 4.640541 Tokens per Sec: 836.577884\n",
            "Epoch Step: 8251 Loss: 4.401945 Tokens per Sec: 834.115801\n",
            "Epoch Step: 8301 Loss: 2.858346 Tokens per Sec: 795.414314\n",
            "Epoch Step: 8351 Loss: 4.332211 Tokens per Sec: 846.649276\n",
            "Epoch Step: 8401 Loss: 3.477866 Tokens per Sec: 850.744574\n",
            "Epoch Step: 8451 Loss: 4.288407 Tokens per Sec: 808.814297\n",
            "Epoch Step: 8501 Loss: 4.400078 Tokens per Sec: 793.581823\n",
            "Epoch Step: 8551 Loss: 4.524117 Tokens per Sec: 833.781304\n",
            "Epoch Step: 8601 Loss: 4.527191 Tokens per Sec: 834.940709\n",
            "Epoch Step: 8651 Loss: 4.272499 Tokens per Sec: 834.484787\n",
            "Epoch Step: 8701 Loss: 4.085607 Tokens per Sec: 841.295941\n",
            "Epoch Step: 8751 Loss: 4.037576 Tokens per Sec: 868.707362\n",
            "Epoch Step: 8801 Loss: 4.004466 Tokens per Sec: 871.196692\n",
            "Epoch Step: 8851 Loss: 4.012332 Tokens per Sec: 825.962619\n",
            "Epoch Step: 8901 Loss: 4.471797 Tokens per Sec: 861.668380\n",
            "Epoch Step: 8951 Loss: 4.632360 Tokens per Sec: 812.420675\n",
            "Epoch Step: 9001 Loss: 4.393937 Tokens per Sec: 822.598470\n",
            "Epoch Step: 9051 Loss: 4.162911 Tokens per Sec: 799.746757\n",
            "Epoch Step: 9101 Loss: 4.508200 Tokens per Sec: 853.061940\n",
            "Epoch Step: 9151 Loss: 4.399397 Tokens per Sec: 799.186697\n",
            "Epoch Step: 9201 Loss: 3.837020 Tokens per Sec: 809.742530\n",
            "Epoch Step: 9251 Loss: 4.368229 Tokens per Sec: 836.597450\n",
            "Epoch Step: 9301 Loss: 3.743136 Tokens per Sec: 812.186031\n",
            "Epoch Step: 9351 Loss: 4.500424 Tokens per Sec: 861.206255\n",
            "Epoch Step: 9401 Loss: 5.080628 Tokens per Sec: 791.016785\n",
            "Epoch Step: 9451 Loss: 4.470795 Tokens per Sec: 862.545936\n",
            "Epoch Step: 9501 Loss: 3.840494 Tokens per Sec: 841.078317\n",
            "Epoch Step: 9551 Loss: 4.355113 Tokens per Sec: 804.923155\n",
            "Epoch Step: 9601 Loss: 4.077807 Tokens per Sec: 808.705635\n",
            "Epoch Step: 9651 Loss: 4.266002 Tokens per Sec: 817.606746\n",
            "Epoch Step: 9701 Loss: 4.006300 Tokens per Sec: 839.942766\n",
            "Epoch Step: 9751 Loss: 4.093043 Tokens per Sec: 835.612551\n",
            "Epoch Step: 9801 Loss: 4.299982 Tokens per Sec: 850.753491\n",
            "Epoch Step: 9851 Loss: 4.041627 Tokens per Sec: 818.936375\n",
            "Epoch Step: 9901 Loss: 4.541195 Tokens per Sec: 834.089630\n",
            "Epoch Step: 9951 Loss: 4.028270 Tokens per Sec: 814.784325\n",
            "Epoch Step: 10001 Loss: 4.472578 Tokens per Sec: 834.667614\n",
            "Epoch Step: 10051 Loss: 4.691121 Tokens per Sec: 857.352004\n",
            "Epoch Step: 10101 Loss: 4.441843 Tokens per Sec: 833.351005\n",
            "Epoch Step: 10151 Loss: 4.183581 Tokens per Sec: 853.539333\n",
            "Epoch Step: 10201 Loss: 4.606701 Tokens per Sec: 840.749063\n",
            "Epoch Step: 10251 Loss: 4.214918 Tokens per Sec: 826.332518\n",
            "Epoch Step: 10301 Loss: 4.303659 Tokens per Sec: 868.052363\n",
            "Epoch Step: 10351 Loss: 4.595125 Tokens per Sec: 815.454114\n",
            "Epoch Step: 10401 Loss: 4.466417 Tokens per Sec: 818.507212\n",
            "Epoch Step: 10451 Loss: 4.435780 Tokens per Sec: 819.551292\n",
            "Epoch Step: 10501 Loss: 4.417466 Tokens per Sec: 834.046696\n",
            "Epoch Step: 10551 Loss: 3.822879 Tokens per Sec: 811.635644\n",
            "Epoch Step: 10601 Loss: 4.403988 Tokens per Sec: 824.401893\n",
            "Epoch Step: 10651 Loss: 4.495594 Tokens per Sec: 813.468707\n",
            "Epoch Step: 10701 Loss: 4.415339 Tokens per Sec: 864.560151\n",
            "Epoch Step: 10751 Loss: 4.590755 Tokens per Sec: 808.704333\n",
            "Epoch Step: 10801 Loss: 4.177874 Tokens per Sec: 803.201014\n",
            "Epoch Step: 10851 Loss: 4.518534 Tokens per Sec: 846.069403\n",
            "Epoch Step: 10901 Loss: 4.724695 Tokens per Sec: 801.758033\n",
            "Epoch Step: 10951 Loss: 4.229601 Tokens per Sec: 837.335728\n",
            "Epoch Step: 11001 Loss: 4.501954 Tokens per Sec: 811.552645\n",
            "Epoch Step: 11051 Loss: 4.285739 Tokens per Sec: 834.169447\n",
            "Epoch Step: 11101 Loss: 4.167175 Tokens per Sec: 781.390453\n",
            "Epoch Step: 11151 Loss: 4.409289 Tokens per Sec: 846.442990\n",
            "Epoch Step: 11201 Loss: 4.247526 Tokens per Sec: 854.429926\n",
            "Epoch Step: 11251 Loss: 3.857310 Tokens per Sec: 807.182048\n",
            "Epoch Step: 11301 Loss: 4.202765 Tokens per Sec: 810.866620\n",
            "Epoch Step: 11351 Loss: 4.252257 Tokens per Sec: 829.834418\n",
            "Epoch Step: 11401 Loss: 4.314391 Tokens per Sec: 828.763402\n",
            "Epoch Step: 11451 Loss: 3.769198 Tokens per Sec: 838.925963\n",
            "Epoch Step: 11501 Loss: 3.642078 Tokens per Sec: 817.061560\n",
            "Epoch Step: 11551 Loss: 4.079615 Tokens per Sec: 830.861847\n",
            "Epoch Step: 11601 Loss: 4.046179 Tokens per Sec: 829.316857\n",
            "Epoch Step: 11651 Loss: 3.554085 Tokens per Sec: 847.843206\n",
            "Epoch Step: 11701 Loss: 4.190409 Tokens per Sec: 859.135610\n",
            "Epoch Step: 11751 Loss: 4.438283 Tokens per Sec: 799.000901\n",
            "Epoch Step: 11801 Loss: 4.624674 Tokens per Sec: 831.596580\n",
            "Epoch Step: 11851 Loss: 4.301561 Tokens per Sec: 867.273306\n",
            "Epoch Step: 11901 Loss: 4.570963 Tokens per Sec: 845.840696\n",
            "Epoch Step: 11951 Loss: 3.891681 Tokens per Sec: 833.415529\n",
            "Epoch Step: 12001 Loss: 3.939202 Tokens per Sec: 843.952929\n",
            "Epoch Step: 12051 Loss: 4.505394 Tokens per Sec: 786.414127\n",
            "Epoch Step: 12101 Loss: 4.188455 Tokens per Sec: 865.985595\n",
            "Epoch Step: 12151 Loss: 4.396540 Tokens per Sec: 832.446077\n",
            "Epoch Step: 12201 Loss: 4.426338 Tokens per Sec: 834.706183\n",
            "Epoch Step: 12251 Loss: 3.756929 Tokens per Sec: 852.200120\n",
            "Epoch Step: 12301 Loss: 4.154206 Tokens per Sec: 821.027771\n",
            "Epoch Step: 12351 Loss: 3.940509 Tokens per Sec: 780.963630\n",
            "Epoch Step: 12401 Loss: 5.052660 Tokens per Sec: 852.660759\n",
            "Epoch Step: 12451 Loss: 4.267617 Tokens per Sec: 842.444025\n",
            "Epoch Step: 12501 Loss: 4.275112 Tokens per Sec: 814.936660\n",
            "Epoch Step: 12551 Loss: 4.695941 Tokens per Sec: 823.297612\n",
            "Epoch Step: 12601 Loss: 3.799292 Tokens per Sec: 805.814933\n",
            "Epoch Step: 12651 Loss: 3.957091 Tokens per Sec: 840.137127\n",
            "Epoch Step: 12701 Loss: 4.657937 Tokens per Sec: 868.170157\n",
            "Epoch Step: 12751 Loss: 4.730248 Tokens per Sec: 808.853278\n",
            "Epoch Step: 12801 Loss: 4.552123 Tokens per Sec: 828.237142\n",
            "Epoch Step: 12851 Loss: 3.994909 Tokens per Sec: 814.054164\n",
            "Epoch Step: 12901 Loss: 4.272154 Tokens per Sec: 841.953798\n",
            "Epoch Step: 12951 Loss: 3.978493 Tokens per Sec: 832.195695\n",
            "Epoch Step: 13001 Loss: 4.332541 Tokens per Sec: 806.142930\n",
            "Epoch Step: 13051 Loss: 4.196471 Tokens per Sec: 847.076194\n",
            "Epoch Step: 13101 Loss: 4.367610 Tokens per Sec: 826.300152\n",
            "Epoch Step: 13151 Loss: 4.128303 Tokens per Sec: 812.526638\n",
            "Epoch Step: 13201 Loss: 4.552824 Tokens per Sec: 840.320402\n",
            "Epoch Step: 13251 Loss: 4.232150 Tokens per Sec: 842.810968\n",
            "Epoch Step: 13301 Loss: 3.979738 Tokens per Sec: 840.254010\n",
            "Epoch Step: 13351 Loss: 4.666622 Tokens per Sec: 825.276745\n",
            "Epoch Step: 13401 Loss: 4.744648 Tokens per Sec: 840.785283\n",
            "Epoch Step: 13451 Loss: 3.960190 Tokens per Sec: 869.068906\n",
            "Epoch Step: 13501 Loss: 4.958497 Tokens per Sec: 834.962335\n",
            "Epoch Step: 13551 Loss: 3.965026 Tokens per Sec: 840.410972\n",
            "Epoch Step: 13601 Loss: 4.656741 Tokens per Sec: 812.155616\n",
            "Epoch Step: 13651 Loss: 4.497375 Tokens per Sec: 809.208273\n",
            "Epoch Step: 13701 Loss: 4.446356 Tokens per Sec: 837.642364\n",
            "Epoch Step: 13751 Loss: 4.265819 Tokens per Sec: 859.127337\n",
            "Epoch Step: 13801 Loss: 4.305411 Tokens per Sec: 826.669818\n",
            "Epoch Step: 13851 Loss: 4.382991 Tokens per Sec: 841.561175\n",
            "Epoch Step: 13901 Loss: 4.609828 Tokens per Sec: 832.894806\n",
            "Epoch Step: 13951 Loss: 3.988970 Tokens per Sec: 826.875077\n",
            "Epoch Step: 14001 Loss: 4.288978 Tokens per Sec: 836.289140\n",
            "Epoch Step: 14051 Loss: 3.773436 Tokens per Sec: 799.940261\n",
            "Epoch Step: 14101 Loss: 4.388530 Tokens per Sec: 820.907034\n",
            "Epoch Step: 14151 Loss: 3.732360 Tokens per Sec: 830.789151\n",
            "Epoch Step: 14201 Loss: 4.517019 Tokens per Sec: 844.472080\n",
            "Epoch Step: 14251 Loss: 4.068882 Tokens per Sec: 838.422302\n",
            "Epoch Step: 14301 Loss: 4.100521 Tokens per Sec: 809.010937\n",
            "Epoch Step: 14351 Loss: 4.352508 Tokens per Sec: 823.950375\n",
            "Epoch Step: 14401 Loss: 4.506626 Tokens per Sec: 851.866733\n",
            "Epoch Step: 14451 Loss: 4.007854 Tokens per Sec: 775.226931\n",
            "Epoch Step: 14501 Loss: 4.379001 Tokens per Sec: 805.045262\n",
            "Epoch Step: 14551 Loss: 4.398195 Tokens per Sec: 832.662681\n",
            "Epoch Step: 14601 Loss: 4.581450 Tokens per Sec: 846.168691\n",
            "Epoch Step: 14651 Loss: 4.128090 Tokens per Sec: 775.857973\n",
            "Epoch Step: 14701 Loss: 4.389771 Tokens per Sec: 829.114506\n",
            "Epoch Step: 14751 Loss: 4.450943 Tokens per Sec: 823.910743\n",
            "Epoch Step: 14801 Loss: 3.561377 Tokens per Sec: 832.343317\n",
            "Epoch Step: 14851 Loss: 3.769210 Tokens per Sec: 788.512780\n",
            "Epoch Step: 14901 Loss: 4.441922 Tokens per Sec: 837.256494\n",
            "Epoch Step: 14951 Loss: 4.435981 Tokens per Sec: 811.416675\n",
            "Epoch Step: 15001 Loss: 4.155915 Tokens per Sec: 814.260076\n",
            "Epoch Step: 15051 Loss: 4.639170 Tokens per Sec: 773.107380\n",
            "Epoch Step: 1 Loss: 0.873364 Tokens per Sec: 530.606031\n",
            "Epoch Step: 51 Loss: 1.248307 Tokens per Sec: 2339.215430\n",
            "Epoch Step: 101 Loss: 1.686005 Tokens per Sec: 2318.882295\n",
            "Epoch Step: 151 Loss: 1.013233 Tokens per Sec: 2345.812775\n",
            "Epoch Step: 201 Loss: 2.026181 Tokens per Sec: 2380.569217\n",
            "Epoch Step: 251 Loss: 1.068560 Tokens per Sec: 2680.707146\n",
            "Epoch Step: 301 Loss: 2.889740 Tokens per Sec: 2764.483133\n",
            "Epoch Step: 351 Loss: 1.318648 Tokens per Sec: 2901.145349\n",
            "Epoch Step: 401 Loss: 3.681557 Tokens per Sec: 2944.353086\n",
            "Epoch Step: 451 Loss: 1.085846 Tokens per Sec: 2857.992789\n",
            "Epoch Step: 501 Loss: 3.834460 Tokens per Sec: 2968.140768\n",
            "Epoch Step: 551 Loss: 3.470905 Tokens per Sec: 2817.168634\n",
            "Epoch Step: 601 Loss: 2.633264 Tokens per Sec: 3033.816776\n",
            "Epoch Step: 651 Loss: 3.499260 Tokens per Sec: 2982.435793\n",
            "Epoch Step: 701 Loss: 3.685981 Tokens per Sec: 3073.970551\n",
            "Epoch Step: 751 Loss: 4.034433 Tokens per Sec: 3122.055506\n",
            "Epoch Step: 801 Loss: 3.261319 Tokens per Sec: 2946.962541\n",
            "Epoch Step: 851 Loss: 4.331824 Tokens per Sec: 3106.357448\n",
            "Epoch Step: 901 Loss: 4.216833 Tokens per Sec: 2877.502565\n",
            "Epoch Step: 951 Loss: 4.034222 Tokens per Sec: 3134.495504\n",
            "Epoch Step: 1001 Loss: 3.845294 Tokens per Sec: 3099.653726\n",
            "Epoch Step: 1051 Loss: 3.453703 Tokens per Sec: 2937.645279\n",
            "Epoch Step: 1101 Loss: 4.103208 Tokens per Sec: 3100.822963\n",
            "Epoch Step: 1151 Loss: 3.670113 Tokens per Sec: 3066.008160\n",
            "Epoch Step: 1201 Loss: 3.839070 Tokens per Sec: 3141.784348\n",
            "Epoch Step: 1251 Loss: 3.949195 Tokens per Sec: 2909.249349\n",
            "Epoch Step: 1301 Loss: 3.892860 Tokens per Sec: 3065.613250\n",
            "Epoch Step: 1351 Loss: 4.184713 Tokens per Sec: 3152.394366\n",
            "Epoch Step: 1401 Loss: 4.324236 Tokens per Sec: 2985.068382\n",
            "Epoch Step: 1451 Loss: 4.217383 Tokens per Sec: 2960.695496\n",
            "Epoch Step: 1501 Loss: 4.323274 Tokens per Sec: 3138.820772\n",
            "Epoch Step: 1551 Loss: 3.547513 Tokens per Sec: 3131.550796\n",
            "Epoch Step: 1601 Loss: 3.914493 Tokens per Sec: 3132.438127\n",
            "Epoch Step: 1651 Loss: 4.133269 Tokens per Sec: 2823.492284\n",
            "Epoch Step: 1701 Loss: 3.578362 Tokens per Sec: 3125.503390\n",
            "Epoch Step: 1751 Loss: 3.825344 Tokens per Sec: 3107.197856\n",
            "Epoch Step: 1801 Loss: 4.148828 Tokens per Sec: 3052.035770\n",
            "Epoch Step: 1851 Loss: 4.094375 Tokens per Sec: 2809.204934\n",
            "Epoch Step: 1901 Loss: 4.238855 Tokens per Sec: 3066.762737\n",
            "Epoch Step: 1951 Loss: 4.034997 Tokens per Sec: 3125.443150\n",
            "Epoch Step: 2001 Loss: 3.700749 Tokens per Sec: 3082.084902\n",
            "Epoch Step: 2051 Loss: 4.201744 Tokens per Sec: 3144.339147\n",
            "Epoch Step: 2101 Loss: 3.824456 Tokens per Sec: 2851.639609\n",
            "Epoch Step: 2151 Loss: 4.159504 Tokens per Sec: 3043.899484\n",
            "Epoch Step: 2201 Loss: 3.878616 Tokens per Sec: 3118.760423\n",
            "Epoch Step: 2251 Loss: 4.189518 Tokens per Sec: 3084.560347\n",
            "Epoch Step: 2301 Loss: 3.959897 Tokens per Sec: 3155.752640\n",
            "Epoch Step: 2351 Loss: 4.091462 Tokens per Sec: 2886.893418\n",
            "Epoch Step: 2401 Loss: 4.400826 Tokens per Sec: 2937.021182\n",
            "Epoch Step: 2451 Loss: 4.152645 Tokens per Sec: 3110.225410\n",
            "Epoch Step: 2501 Loss: 4.193205 Tokens per Sec: 3255.772493\n",
            "Epoch Step: 2551 Loss: 4.202671 Tokens per Sec: 3151.179714\n",
            "Epoch Step: 2601 Loss: 4.025640 Tokens per Sec: 3186.326519\n",
            "Epoch Step: 2651 Loss: 3.846451 Tokens per Sec: 2615.952508\n",
            "Epoch Step: 2701 Loss: 4.250203 Tokens per Sec: 3059.691726\n",
            "Epoch Step: 2751 Loss: 3.803092 Tokens per Sec: 3163.261226\n",
            "Epoch Step: 2801 Loss: 4.403330 Tokens per Sec: 3169.052807\n",
            "Epoch Step: 2851 Loss: 3.814371 Tokens per Sec: 3189.529875\n",
            "Epoch Step: 2901 Loss: 4.353592 Tokens per Sec: 2850.444700\n",
            "Epoch Step: 2951 Loss: 3.547501 Tokens per Sec: 3061.981668\n",
            "Epoch Step: 3001 Loss: 3.967944 Tokens per Sec: 3150.135885\n",
            "Epoch Step: 3051 Loss: 3.924387 Tokens per Sec: 3278.230522\n",
            "Epoch Step: 3101 Loss: 4.297534 Tokens per Sec: 3201.905063\n",
            "Epoch Step: 3151 Loss: 4.840767 Tokens per Sec: 2965.892188\n",
            "Epoch Step: 3201 Loss: 3.749552 Tokens per Sec: 2819.110433\n",
            "Epoch Step: 3251 Loss: 4.421509 Tokens per Sec: 3122.916354\n",
            "Epoch Step: 3301 Loss: 4.067579 Tokens per Sec: 3240.901072\n",
            "Epoch Step: 3351 Loss: 3.997955 Tokens per Sec: 3193.123047\n",
            "Epoch Step: 3401 Loss: 4.142034 Tokens per Sec: 3137.046139\n",
            "Epoch Step: 3451 Loss: 3.938737 Tokens per Sec: 2839.547379\n",
            "Epoch Step: 3501 Loss: 4.372829 Tokens per Sec: 3123.426785\n",
            "Epoch Step: 3551 Loss: 4.017251 Tokens per Sec: 3207.399395\n",
            "Epoch Step: 3601 Loss: 3.966431 Tokens per Sec: 3250.809392\n",
            "Epoch Step: 3651 Loss: 3.935164 Tokens per Sec: 3085.073226\n",
            "Epoch Step: 3701 Loss: 4.369340 Tokens per Sec: 3111.834945\n",
            "Epoch Step: 3751 Loss: 4.124048 Tokens per Sec: 2590.126203\n",
            "Epoch Step: 3801 Loss: 4.235675 Tokens per Sec: 2989.729654\n",
            "Epoch Step: 3851 Loss: 4.153851 Tokens per Sec: 3208.247447\n",
            "Epoch Step: 3901 Loss: 4.010230 Tokens per Sec: 3053.296923\n",
            "Epoch Step: 3951 Loss: 3.787532 Tokens per Sec: 3188.908034\n",
            "Epoch Step: 4001 Loss: 4.442624 Tokens per Sec: 2810.906253\n",
            "Epoch Step: 4051 Loss: 4.434545 Tokens per Sec: 2932.019388\n",
            "Epoch Step: 4101 Loss: 4.180928 Tokens per Sec: 3259.439851\n",
            "Epoch Step: 4151 Loss: 4.135987 Tokens per Sec: 3129.061419\n",
            "Epoch Step: 4201 Loss: 4.149089 Tokens per Sec: 3280.001565\n",
            "Epoch Step: 4251 Loss: 3.563693 Tokens per Sec: 3115.131361\n",
            "Epoch Step: 4301 Loss: 3.911185 Tokens per Sec: 2514.511354\n",
            "Epoch Step: 4351 Loss: 4.405373 Tokens per Sec: 2807.224154\n",
            "Epoch Step: 4401 Loss: 4.631194 Tokens per Sec: 3071.544892\n",
            "Epoch Step: 4451 Loss: 4.293559 Tokens per Sec: 3269.760317\n",
            "Epoch Step: 4501 Loss: 4.255856 Tokens per Sec: 3134.950580\n",
            "Epoch Step: 4551 Loss: 3.892585 Tokens per Sec: 3108.613243\n",
            "Epoch Step: 4601 Loss: 4.137995 Tokens per Sec: 2659.725994\n",
            "Epoch Step: 4651 Loss: 4.545367 Tokens per Sec: 3033.933709\n",
            "Epoch Step: 4701 Loss: 4.312535 Tokens per Sec: 3286.474902\n",
            "Epoch Step: 4751 Loss: 4.170406 Tokens per Sec: 3085.867543\n",
            "Epoch Step: 4801 Loss: 3.849303 Tokens per Sec: 3138.071569\n",
            "Epoch Step: 4851 Loss: 4.311829 Tokens per Sec: 2793.446503\n",
            "Epoch Step: 4901 Loss: 4.254890 Tokens per Sec: 3050.838833\n",
            "Epoch Step: 4951 Loss: 4.218585 Tokens per Sec: 3223.744532\n",
            "Epoch Step: 5001 Loss: 4.308997 Tokens per Sec: 3140.673207\n",
            "Epoch Step: 5051 Loss: 4.415546 Tokens per Sec: 3066.557312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4233317a3b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                           SimpleLossCompute(model.generator, criterion, \n\u001b[0;32m---> 16\u001b[0;31m                            opt=None))\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'transformer'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d4e5053f9cfa>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[0;32m----> 9\u001b[0;31m                             batch.src_mask, batch.trg_mask)\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-72b45e2d6e1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m\"Take in and process masked src and target sequences.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         return self.decode(self.encode(src, src_mask), src_mask,\n\u001b[0;32m---> 17\u001b[0;31m                             tgt, tgt_mask)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-72b45e2d6e1e>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, memory, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-72b45e2d6e1e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PPxOESGVT659",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!free -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZdLl3jC4Q2Po",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Translation"
      ]
    },
    {
      "metadata": {
        "id": "4Lou3o8JQ3mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "83ce6c06-3ae0-4572-9f19-dfa0a7dcf0ed"
      },
      "cell_type": "code",
      "source": [
        "## Load the model\n",
        "model = torch.load(\"transformer0.pt\")\n",
        "\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translation:\t|v4 \n",
            "Target:\t27. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g3FW_tAxoxHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}